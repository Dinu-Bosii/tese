{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\pt_venv2\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\pt_venv2\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from snn_model import get_loss_fn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp,smiles_to_descriptor,smiles_to_onehot, smiles_to_onehot_selfies, data_splitter, get_spiking_net, make_filename\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from csnn_model import CSNNet, get_prediction_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[0]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "    # SR-MMP\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders 1427 samples, 0.52 class ratio\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Molecular Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"fp\", \"descriptor\", \"SELFIES-1hot\", \"SMILES-1hot\"]#, \"graph-list\"]\n",
    "\n",
    "repr_type = representations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptor\n"
     ]
    }
   ],
   "source": [
    "if repr_type == \"fp\":\n",
    "    fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['count_morgan', 1024], ['pubchem', 881]]\n",
    "    mix = True\n",
    "    fp_type, num_bits = fp_types[1]\n",
    "    if mix and fp_type == 'RDKit':\n",
    "        num_bits = 512\n",
    "    data_config = {\"fp_type\": fp_type,\n",
    "                \"num_bits\": num_bits,\n",
    "                \"radius\": 2,\n",
    "                \"fp_type_2\": fp_types[0][0],\n",
    "                \"num_bits_2\": 1024 - num_bits,\n",
    "                \"mix\": mix,}\n",
    "    dim_2 = False\n",
    "    print(fp_type, '-', num_bits)\n",
    "    if mix: print(data_config['fp_type_2'], '-', data_config['num_bits_2'])\n",
    "    if dim_2: print(\"2D FP\")\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    desc_type = [\"RDKit\", \"TODO\"]\n",
    "    data_config = {\"desc\": desc_type[0],\n",
    "                   \"size\": 0,\n",
    "                }\n",
    "elif repr_type == \"SELFIES-1hot\":\n",
    "    dim_2 = True\n",
    "    data_config = {}\n",
    "\n",
    "elif repr_type == \"SMILES-1hot\":\n",
    "    dim_2 = True\n",
    "    data_config = {}\n",
    "\n",
    "data_config[\"repr_type\"] = repr_type\n",
    "print(repr_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inf in descriptor MaxPartialCharge for molecule CC1=C2N=C(C=C3N=C(C(C)=C4[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@](C)([C@@H]5N=C1[C@](C)(CCC(=O)NC[C@@H](C)OP(=O)([O-])O[C@@H]1[C@@H](CO)O[C@H](n6cnc7cc(C)c(C)cc76)[C@@H]1O)[C@H]5CC(N)=O)N4[Co+]C#N)[C@@](C)(CC(N)=O)[C@@H]3CCC(N)=O)C(C)(C)[C@@H]2CCC(N)=O\n",
      "Inf in descriptor MinPartialCharge for molecule CC1=C2N=C(C=C3N=C(C(C)=C4[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@](C)([C@@H]5N=C1[C@](C)(CCC(=O)NC[C@@H](C)OP(=O)([O-])O[C@@H]1[C@@H](CO)O[C@H](n6cnc7cc(C)c(C)cc76)[C@@H]1O)[C@H]5CC(N)=O)N4[Co+]C#N)[C@@](C)(CC(N)=O)[C@@H]3CCC(N)=O)C(C)(C)[C@@H]2CCC(N)=O\n",
      "Inf in descriptor MaxAbsPartialCharge for molecule CC1=C2N=C(C=C3N=C(C(C)=C4[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@](C)([C@@H]5N=C1[C@](C)(CCC(=O)NC[C@@H](C)OP(=O)([O-])O[C@@H]1[C@@H](CO)O[C@H](n6cnc7cc(C)c(C)cc76)[C@@H]1O)[C@H]5CC(N)=O)N4[Co+]C#N)[C@@](C)(CC(N)=O)[C@@H]3CCC(N)=O)C(C)(C)[C@@H]2CCC(N)=O\n",
      "Inf in descriptor MinAbsPartialCharge for molecule CC1=C2N=C(C=C3N=C(C(C)=C4[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@](C)([C@@H]5N=C1[C@](C)(CCC(=O)NC[C@@H](C)OP(=O)([O-])O[C@@H]1[C@@H](CO)O[C@H](n6cnc7cc(C)c(C)cc76)[C@@H]1O)[C@H]5CC(N)=O)N4[Co+]C#N)[C@@](C)(CC(N)=O)[C@@H]3CCC(N)=O)C(C)(C)[C@@H]2CCC(N)=O\n",
      "Inf in descriptor MaxPartialCharge for molecule CCCCCCCCCCCC(=O)O[Sn](CCCC)(CCCC)OC(=O)CCCCCCCCCCC\n",
      "Inf in descriptor MaxAbsPartialCharge for molecule CCCCCCCCCCCC(=O)O[Sn](CCCC)(CCCC)OC(=O)CCCCCCCCCCC\n",
      "torch.Size([5832, 210])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "split = \"scaffold\"\n",
    "dataset = None\n",
    "\n",
    "if dirname != 'BBBP':\n",
    "    split = \"random\"\n",
    "    if repr_type == \"fp\":\n",
    "        fp_array, target_array = smile_to_fp(df, data_config=data_config, target_name=target_name)\n",
    "        # Create Torch Dataset\n",
    "        fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "        print(fp_tensor.size())\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "        if dim_2:\n",
    "            fp_tensor = fp_tensor.view(-1, 32, 32)\n",
    "            print(fp_tensor.size())\n",
    "        dataset = TensorDataset(fp_tensor, target_tensor)\n",
    "    elif repr_type == \"descriptor\":\n",
    "        desc_array, target_array = smiles_to_descriptor(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        desc_tensor = torch.tensor(desc_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(desc_tensor, target_tensor)\n",
    "        print(desc_tensor.size())\n",
    "    elif repr_type == \"SELFIES-1hot\":\n",
    "        selfies_array, target_array = smiles_to_onehot_selfies(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        selfies_tensor = torch.tensor(selfies_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(selfies_tensor, target_tensor)\n",
    "        print(selfies_tensor.size())\n",
    "    elif repr_type == \"SMILES-1hot\":\n",
    "        smiles_array, target_array = smiles_to_onehot(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        smiles_tensor = torch.tensor(smiles_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(smiles_tensor, target_tensor)\n",
    "        print(smiles_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_loss\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "loss_types = ['ce_mem', 'rate_loss', 'count_loss', 'temporal_loss', 'bce_loss']\n",
    "loss_type = loss_types[2]\n",
    "print(loss_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSNN\n"
     ]
    }
   ],
   "source": [
    "net_types = [\"SNN\", \"DSNN\", \"CSNN\", \"RSNN\"]\n",
    "net_type = net_types[1]\n",
    "slope = 10\n",
    "#spike_grad = surrogate.fast_sigmoid(slope=slope)\n",
    "spike_grad = None\n",
    "beta = 0.95 \n",
    "bias = True\n",
    "net_config = {\n",
    "            \"num_hidden\": 512,\n",
    "            \"num_hidden_l2\": 256,\n",
    "            \"num_steps\": 10,\n",
    "            \"spike_grad\": spike_grad,\n",
    "            \"slope\": None if not spike_grad else slope, #spike_grad.__closure__[0].cell_contents,\n",
    "            \"beta\": beta,\n",
    "            \"encoding\": 'rate' if loss_type != 'temporal_loss' else 'ttfs',\n",
    "            \"bias\": bias,\n",
    "            \"out_num\": 2\n",
    "            }\n",
    "if net_type == \"CSNN\":\n",
    "    net_config['num_conv'] = 1\n",
    "    net_config['stride'] = [1 for _ in range(net_config['num_conv'])]\n",
    "    net_config[\"pool_size\"] = 2\n",
    "    net_config[\"conv_kernel\"] = 3\n",
    "    net_config[\"conv_stride\"] = 1\n",
    "    net_config[\"conv_groups\"] = 1\n",
    "\n",
    "if repr_type == \"fp\":\n",
    "    net_config[\"input_size\"] = 1024 if data_config['mix'] else num_bits\n",
    "    net_config[\"2d\"] = dim_2\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    net_config[\"input_size\"] = desc_tensor.shape[1]\n",
    "    net_config[\"2d\"] = False\n",
    "    net_config[\"time_steps\"] = 50\n",
    "\n",
    "if repr_type == \"SELFIES-1hot\":\n",
    "    net_config[\"input_size\"] = [desc_tensor.shape[1],desc_tensor.shape[2]] \n",
    "    net_config[\"2d\"] = True\n",
    "if repr_type == \"SMILES-1hot\":\n",
    "    net_config[\"2d\"] = True\n",
    "    net_config[\"input_size\"] = [desc_tensor.shape[1],desc_tensor.shape[2]] \n",
    "print(net_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "pop_coding = net_config['out_num'] > 2\n",
    "lr=1e-4 #1e-6 default for 1000 epochs. csnn requires higher\n",
    "iterations = 30\n",
    "weight_decay = 0 # 1e-5\n",
    "optim_type = 'Adam'\n",
    "#optim_type = 'SGD'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "batch_size = 16 #16, 8\n",
    "train_config = {\"num_epochs\": 50,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['num_steps'],\n",
    "                'val_net': None,\n",
    "                'prediction_fn': get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding),\n",
    "                }\n",
    "drop_last = net_type == \"CSNN\"\n",
    "pin_memory = device == \"cuda\"\n",
    "save_csv = True\n",
    "save_models = True\n",
    "results = [[], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(metrics_list, all_targets, all_preds):\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    \n",
    "    print(\"acc:\", accuracy,\"auc:\", auc_roc)\n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 512\n",
      "512 256\n",
      "256 2\n",
      "results\\tox21\\models\\\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-1.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-2.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-3.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-4.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-5.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-6.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-7.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-8.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-9.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-10.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-11.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-12.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-13.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-14.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-15.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-16.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-17.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-18.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-19.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-20.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-21.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-22.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-23.pth\n",
      "SR-ARE_DSNN_beta-0.95_desc_210_l1512_l2256_t10_e50_b16_lr0.0001_count_loss_Adam_wd0_biasseed-24.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "net_list = []\n",
    "    \n",
    "net, train_net, val_net, test_net = get_spiking_net(net_type, net_config)\n",
    "filename = make_filename(dirname, target_name, net_type, data_config, lr, weight_decay, optim_type, net_config, train_config, net, model = True)\n",
    "\n",
    "model_name = filename.removesuffix('.csv')\n",
    "\n",
    "models_path = os.path.join(\"results\", dirname, \"models\", \"\")\n",
    "all_model_names = os.listdir(models_path)\n",
    "print(models_path)\n",
    "#print(all_model_names)\n",
    "for iter in range(iterations):\n",
    "    seed = int(iter + 1)\n",
    "    string_id = f\"seed-{seed}.pth\"\n",
    "    search_name = model_name + str(string_id) \n",
    "    search_name_no_folder = search_name.removeprefix(models_path)\n",
    "    if search_name_no_folder in all_model_names:\n",
    "        state_dict = torch.load(search_name, weights_only=True)\n",
    "        net_list.append(copy.deepcopy(state_dict))\n",
    "    else: print(search_name_no_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "def make_filename(dirname, target, net_type, data_config, lr, wd, optim_type, net_config, train_config, net, model = False):\n",
    "    results_dir = os.path.join(\"results\", dirname, \"\")\n",
    "    if model:\n",
    "        results_dir = os.path.join(results_dir, \"models\", \"\")\n",
    "\n",
    "    data_str = [] \n",
    "    if data_config[\"repr_type\"] == 'descriptor':\n",
    "        data_str.append(\"desc\")\n",
    "    elif data_config[\"repr_type\"] == 'fp':\n",
    "        data_str.append(data_config['fp_type'])\n",
    "        if data_config['fp_type'] == 'morgan':\n",
    "            data_str.append(f\"r-{data_config['radius']}\")\n",
    "        if data_config['mix']:\n",
    "            data_str.append(data_config['fp_type_2'])\n",
    "        if net_config['2d']:\n",
    "            data_str.append(\"2D\")\n",
    "            \n",
    "    input_size = net_config['input_size']\n",
    "    if isinstance(input_size, int):\n",
    "        input_size_l = [input_size]\n",
    "    else: input_size_l = input_size\n",
    "    params = [\n",
    "        None if dirname == 'BBBP' else target, \n",
    "        net_type, \n",
    "        f\"beta-{net_config['beta']}\",\n",
    "        *(\n",
    "            [\"desc\"] if data_config[\"repr_type\"] == 'descriptor' else\n",
    "            (\n",
    "                [data_config['repr_type']] if data_config[\"repr_type\"] != 'fp' else\n",
    "                [data_config['fp_type']] + \n",
    "                (['r-' + f\"{data_config['radius']}\"] if data_config['fp_type'] == 'morgan' else []) +\n",
    "                ([data_config['fp_type_2']] if data_config['mix'] else []) +\n",
    "                ([\"2D\"] if net_config['2d'] else [])\n",
    "            )\n",
    "        ),\n",
    "        *input_size_l,\n",
    "        None if net_type == \"CSNN\" else f\"l1{net_config['num_hidden']}\",\n",
    "        None if net_type != \"DSNN\" else f\"l2{net_config['num_hidden_l2']}\",\n",
    "        None if net_type != \"CSNN\" else \"out-\" + \"-\".join(str(layer.out_channels) for layer in net.layers if isinstance(layer, (nn.Conv1d, nn.Conv2d))),\n",
    "        None if net_type != \"CSNN\" else f\"kernel-{net.conv_kernel}\",\n",
    "        None if net_type != \"CSNN\" else f\"stride-{net.conv_stride}\",\n",
    "        f\"t{net_config['num_steps']}\",\n",
    "        f\"e{train_config['num_epochs']}\",\n",
    "        f\"b{train_config['batch_size']}\",\n",
    "        f\"lr{lr}\",\n",
    "        train_config['loss_type'],\n",
    "        optim_type,\n",
    "        f\"wd{wd}\",\n",
    "        None if net_config['spike_grad'] is None else f\"sig-{net_config['slope']}\",\n",
    "        \"no-bias\" if not net_config['bias'] else \"bias\",\n",
    "        None if net_config['out_num'] == 2 else f\"pop-{net_config['out_num']}\",\n",
    "    ]\n",
    "\n",
    "    filename = results_dir + \"_\".join(str(p) for p in params if p is not None) + \".csv\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:1 -> acc: 0.7221269296740995 auc: 0.4949627823408625\n",
      "\n",
      "Seed:2 -> acc: 0.7324185248713551 auc: 0.5250150693188668\n",
      "\n",
      "Seed:3 -> acc: 0.8593481989708405 auc: 0.5050995569835938\n",
      "\n",
      "Seed:4 -> acc: 0.8078902229845626 auc: 0.49346744934980225\n",
      "\n",
      "Seed:5 -> acc: 0.8456260720411664 auc: 0.49497991967871485\n",
      "\n",
      "Seed:6 -> acc: 0.8439108061749572 auc: 0.5080685147622267\n",
      "\n",
      "Seed:7 -> "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m best_test_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mnet_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m all_preds, all_targets \u001b[38;5;241m=\u001b[39m test_net(model, device, test_loader, train_config)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for iter in range(iterations):\n",
    "    #print(f\"Iteration:{iter + 1}/{iterations}\")\n",
    "    seed = iter + 1\n",
    "    print(f\"Seed:{seed} -> \",end='', flush=True)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # DATA SPLIT\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, data_config=data_config, seed=seed, dtype=dtype)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # TESTING\n",
    "    model = net\n",
    "    best_test_auc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    model.load_state_dict(net_list[iter])\n",
    "    model.to(device)\n",
    "    all_preds, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    calc_metrics(results, all_preds=all_preds, all_targets=all_targets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.802 ± 0.055\n",
      "AUC ROC: 0.504 ± 0.011\n",
      "Sensitivity: 0.073 ± 0.092\n",
      "Specificity: 0.934 ± 0.080\n"
     ]
    }
   ],
   "source": [
    "metrics_np = np.zeros(12)\n",
    "\n",
    "for i, metric in enumerate(results):\n",
    "    metrics_np[i*2] = np.round(np.mean(metric), 3)\n",
    "    metrics_np[i*2+1] = np.round(np.std(metric), 3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy:  {metrics_np[0]:.3f} ± {metrics_np[1]:.3f}\")\n",
    "print(f\"AUC ROC: {metrics_np[2]:.3f} ± {metrics_np[3]:.3f}\")\n",
    "print(f\"Sensitivity: {metrics_np[4]:.3f} ± {metrics_np[5]:.3f}\")\n",
    "print(f\"Specificity: {metrics_np[6]:.3f} ± {metrics_np[7]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSNNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): Leaky()\n",
      "    (2): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): Leaky()\n",
      "    (4): Linear(in_features=2048, out_features=2, bias=True)\n",
      "    (5): Leaky()\n",
      "  )\n",
      "  (fc_out): Linear(in_features=2048, out_features=2, bias=True)\n",
      "  (lif_out): Leaky()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\\tox21\\SR-ARE_CSNN_beta-0.95_maccs_morgan_[1024]_out-8-8_kernel-3_stride-1_t10_e1000_b16_lr0.0001_ce_mem_Adam_wd0_bias.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics_np = metrics_np.reshape(1, -1)\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "num_hidden = net_config['num_hidden']\n",
    "time_steps = train_config['num_steps']\n",
    "num_epochs = train_config['num_epochs']\n",
    "\n",
    "save = True\n",
    "filename = make_filename(dirname, target_name, net_type, data_config, lr, weight_decay, optim_type, net_config, train_config, model)\n",
    "if save: df_metrics.to_csv(filename, index=False)\n",
    "\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
