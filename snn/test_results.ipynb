{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from snn_model import get_loss_fn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from snntorch import spikegen, surrogate\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp_mix, smile_to_fp, data_splitter, get_spiking_net, make_filename\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from csnn_model import CSNNet, get_prediction_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[0]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "    # SR-MMP\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders 1427 samples, 0.52 class ratio\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILE to Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maccs - 167\n"
     ]
    }
   ],
   "source": [
    "fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['pubchem', 881]]\n",
    "mix = True\n",
    "fp_type, num_bits = fp_types[1]\n",
    "if mix: #Always use maccs + Morgan\n",
    "    fp_type, num_bits = fp_types[1]\n",
    "#num_bits = 2048\n",
    "fp_config = {\"fp_type\": fp_type,\n",
    "             \"num_bits\": num_bits,\n",
    "             \"radius\": 2,\n",
    "             \"fp_type_2\": fp_types[0][0],\n",
    "             \"num_bits_2\": 1024 - num_bits,\n",
    "             \"mix\": mix,\n",
    "             }\n",
    "\n",
    "print(fp_type, '-', num_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:57:19] Explicit valence for atom # 8 Al, 6, is greater than permitted\n",
      "[17:57:20] Explicit valence for atom # 3 Al, 6, is greater than permitted\n",
      "[17:57:20] Explicit valence for atom # 4 Al, 6, is greater than permitted\n",
      "[17:57:21] Explicit valence for atom # 4 Al, 6, is greater than permitted\n",
      "[17:57:22] Explicit valence for atom # 9 Al, 6, is greater than permitted\n",
      "[17:57:22] Explicit valence for atom # 5 Al, 6, is greater than permitted\n",
      "[17:57:23] Explicit valence for atom # 16 Al, 6, is greater than permitted\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "dataset = None\n",
    "split = \"random\"\n",
    "\n",
    "if dirname == 'BBBP':\n",
    "    split = \"scaffold\"\n",
    "else:\n",
    "    if fp_config['mix']:\n",
    "        fp_array, target_array = smile_to_fp_mix(df, fp_config=fp_config, target_name=target_name)\n",
    "    else:\n",
    "        fp_array, target_array = smile_to_fp(df, fp_config=fp_config, target_name=target_name)\n",
    "    # Create Torch Dataset\n",
    "    fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "    target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "    dataset = TensorDataset(fp_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5825, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(fp_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_loss\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "loss_types = ['ce_mem', 'rate_loss', 'count_loss', 'temporal_loss', 'bce_loss']\n",
    "loss_type = loss_types[2]\n",
    "print(loss_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_types = [\"SNN\", \"DSNN\", \"CSNN\"]\n",
    "net_type = net_types[2]\n",
    "#spike_grad = surrogate.sigmoid(slope=25)\n",
    "spike_grad = None\n",
    "beta = 0.95 #experimentar 0.7\n",
    "\n",
    "net_config = {\"input_size\": 1024 if fp_config['mix'] else num_bits,\n",
    "              \"num_hidden\": 512,\n",
    "              \"num_hidden_l2\": 256,\n",
    "              \"use_l2\": net_type == \"DSNN\",\n",
    "              \"time_steps\": 10,\n",
    "              \"spike_grad\": spike_grad,\n",
    "              \"beta\": beta,\n",
    "              \"encoding\": 'rate',\n",
    "              \"out_num\": 2\n",
    "              }\n",
    "pop_coding = net_config['out_num'] > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iterations = 30\n",
    "batch_size = 16 #16, 8\n",
    "weight_decay = 0\n",
    "lr = 1e-4\n",
    "optim_type = 'Adam'\n",
    "train_config = {\"num_epochs\": 1000,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['time_steps'],\n",
    "                'val_net': None,\n",
    "                'prediction_fn': get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding),\n",
    "                }\n",
    "drop_last = net_type == \"CSNN\"\n",
    "pin_memory = device == \"cuda\"\n",
    "save = True\n",
    "results = [[], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(metrics_list, all_targets, all_preds):\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    \n",
    "    print(accuracy, auc_roc)\n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\\tox21\\models\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "net_list = []\n",
    "    \n",
    "net, train_net, val_net, test_net = get_spiking_net(net_type, net_config)\n",
    "filename = make_filename(dirname, target_name, net_type, fp_config, lr, weight_decay, optim_type, net_config, train_config, net, model = True)\n",
    "model_name = filename.removesuffix('.csv')\n",
    "\n",
    "models_path = os.path.join(\"results\", dirname, \"models\", \"\")\n",
    "all_model_names = os.listdir(models_path)\n",
    "print(models_path)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    seed = int(iter + 1)\n",
    "    string_id = f\"seed-{seed}.pth\"\n",
    "    search_name = model_name + str(string_id) \n",
    "    search_name_no_folder = search_name.removeprefix(models_path)\n",
    "\n",
    "    if search_name_no_folder in all_model_names:\n",
    "        state_dict = torch.load(search_name, weights_only=True)\n",
    "        net_list.append(copy.deepcopy(state_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:1/30\n",
      "Seed:1\n",
      "0.7869415807560137 0.7381280747617381\n",
      "Iteration:2/30\n",
      "Seed:2\n",
      "0.7508591065292096 0.749569288769078\n",
      "Iteration:3/30\n",
      "Seed:3\n",
      "0.761168384879725 0.7236240975263344\n",
      "Iteration:4/30\n",
      "Seed:4\n",
      "0.8058419243986255 0.7340731707317074\n",
      "Iteration:5/30\n",
      "Seed:5\n",
      "0.7268041237113402 0.7558743760968161\n",
      "Iteration:6/30\n",
      "Seed:6\n",
      "0.7336769759450171 0.7311311876529267\n",
      "Iteration:7/30\n",
      "Seed:7\n",
      "0.7852233676975945 0.7105917104276069\n",
      "Iteration:8/30\n",
      "Seed:8\n",
      "0.718213058419244 0.7156018667524944\n",
      "Iteration:9/30\n",
      "Seed:9\n",
      "0.7628865979381443 0.7279693486590038\n",
      "Iteration:10/30\n",
      "Seed:10\n",
      "0.7972508591065293 0.707534516765286\n",
      "Iteration:11/30\n",
      "Seed:11\n",
      "0.7903780068728522 0.7037403386087596\n",
      "Iteration:12/30\n",
      "Seed:12\n",
      "0.8058419243986255 0.7371061515378845\n",
      "Iteration:13/30\n",
      "Seed:13\n",
      "0.7371134020618557 0.7303582030476945\n",
      "Iteration:14/30\n",
      "Seed:14\n",
      "0.8230240549828178 0.7137533875338753\n",
      "Iteration:15/30\n",
      "Seed:15\n",
      "0.8006872852233677 0.7260159465020577\n",
      "Iteration:16/30\n",
      "Seed:16\n",
      "0.7989690721649485 0.7414887743835112\n",
      "Iteration:17/30\n",
      "Seed:17\n",
      "0.7903780068728522 0.7011929007855687\n",
      "Iteration:18/30\n",
      "Seed:18\n",
      "0.7869415807560137 0.7009751037344397\n",
      "Iteration:19/30\n",
      "Seed:19\n",
      "0.6941580756013745 0.7198097914189994\n",
      "Iteration:20/30\n",
      "Seed:20\n",
      "0.7938144329896907 0.7310341820718521\n",
      "Iteration:21/30\n",
      "Seed:21\n",
      "0.8092783505154639 0.7402798399476559\n",
      "Iteration:22/30\n",
      "Seed:22\n",
      "0.7869415807560137 0.6833333333333332\n",
      "Iteration:23/30\n",
      "Seed:23\n",
      "0.7766323024054983 0.7055441478439426\n",
      "Iteration:24/30\n",
      "Seed:24\n",
      "0.7800687285223368 0.7229437229437229\n",
      "Iteration:25/30\n",
      "Seed:25\n",
      "0.7663230240549829 0.7217658894543733\n",
      "Iteration:26/30\n",
      "Seed:26\n",
      "0.7268041237113402 0.7\n",
      "Iteration:27/30\n",
      "Seed:27\n",
      "0.7869415807560137 0.7304894697970054\n",
      "Iteration:28/30\n",
      "Seed:28\n",
      "0.7800687285223368 0.6995590967077729\n",
      "Iteration:29/30\n",
      "Seed:29\n",
      "0.8092783505154639 0.7482039345583134\n",
      "Iteration:30/30\n",
      "Seed:30\n",
      "0.8350515463917526 0.7487348178137652\n"
     ]
    }
   ],
   "source": [
    "for iter in range(iterations):\n",
    "    print(f\"Iteration:{iter + 1}/{iterations}\")\n",
    "    seed = iter + 1\n",
    "    print(f\"Seed:{seed}\")\n",
    "    random.seed(seed)\n",
    "\n",
    "    # DATA SPLIT\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, fp_config=fp_config, seed=seed, dtype=dtype)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # TESTING\n",
    "    model = net\n",
    "    best_test_auc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    model.load_state_dict(net_list[iter])\n",
    "    model.to(device)\n",
    "    all_preds, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    calc_metrics(results, all_preds=all_preds, all_targets=all_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.777 ± 0.033\n",
      "AUC ROC: 0.723 ± 0.018\n",
      "Sensitivity: 0.646 ± 0.062\n",
      "Specificity: 0.801 ± 0.047\n",
      "results\\tox21\\SR-ARE_CSNN_beta-0.95_maccs_morgan_1024_out-8_kernel-3_stride-1_t10_e1000_b16_lr0.0001_count_loss_Adam_wd0.csv\n"
     ]
    }
   ],
   "source": [
    "metrics_np = np.zeros(12)\n",
    "\n",
    "for i, metric in enumerate(results):\n",
    "    metrics_np[i*2] = np.round(np.mean(metric), 3)\n",
    "    metrics_np[i*2+1] = np.round(np.std(metric), 3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy:  {metrics_np[0]:.3f} ± {metrics_np[1]:.3f}\")\n",
    "print(f\"AUC ROC: {metrics_np[2]:.3f} ± {metrics_np[3]:.3f}\")\n",
    "print(f\"Sensitivity: {metrics_np[4]:.3f} ± {metrics_np[5]:.3f}\")\n",
    "print(f\"Specificity: {metrics_np[6]:.3f} ± {metrics_np[7]:.3f}\")\n",
    "\n",
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics_np = metrics_np.reshape(1, -1)\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "num_hidden = net_config['num_hidden']\n",
    "time_steps = train_config['num_steps']\n",
    "num_epochs = train_config['num_epochs']\n",
    "\n",
    "\n",
    "filename = make_filename(dirname, target_name, net_type, fp_config, lr, weight_decay, optim_type, net_config, train_config, model)\n",
    "if save: df_metrics.to_csv(filename, index=False)\n",
    "\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
