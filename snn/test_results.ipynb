{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from snn_model import get_loss_fn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from snntorch import spikegen, surrogate\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp_mix, smile_to_fp, data_splitter, get_spiking_net, make_filename\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from csnn_model import CSNNet, get_prediction_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[1]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "    # SR-MMP\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders 1427 samples, 0.52 class ratio\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILE to Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maccs - 167\n"
     ]
    }
   ],
   "source": [
    "fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['pubchem', 881]]\n",
    "mix = True\n",
    "fp_type, num_bits = fp_types[0]\n",
    "if mix: #Always use maccs + Morgan\n",
    "    fp_type, num_bits = fp_types[1]\n",
    "#num_bits = 2048\n",
    "fp_config = {\"fp_type\": fp_type,\n",
    "             \"num_bits\": num_bits,\n",
    "             \"radius\": 2,\n",
    "             \"fp_type_2\": fp_types[0][0],\n",
    "             \"num_bits_2\": 1024 - num_bits,\n",
    "             \"mix\": mix,\n",
    "             }\n",
    "\n",
    "print(fp_type, '-', num_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:38:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:38:24] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "dataset = None\n",
    "split = \"random\"\n",
    "\n",
    "if dirname == 'BBBP':\n",
    "    split = \"scaffold\"\n",
    "else:\n",
    "    if fp_config['mix']:\n",
    "        fp_array, target_array = smile_to_fp_mix(df, fp_config=fp_config, target_name=target_name)\n",
    "    else:\n",
    "        fp_array, target_array = smile_to_fp(df, fp_config=fp_config, target_name=target_name)\n",
    "    # Create Torch Dataset\n",
    "    fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "    target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "    dataset = TensorDataset(fp_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1427, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(fp_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_loss\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "loss_types = ['ce_mem', 'rate_loss', 'count_loss', 'temporal_loss', 'bce_loss']\n",
    "loss_type = loss_types[2]\n",
    "print(loss_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_types = [\"SNN\", \"DSNN\", \"CSNN\"]\n",
    "net_type = net_types[2]\n",
    "#spike_grad = surrogate.sigmoid(slope=25)\n",
    "spike_grad = None\n",
    "beta = 0.95 #experimentar 0.7\n",
    "\n",
    "net_config = {\"input_size\": 1024 if fp_config['mix'] else num_bits,\n",
    "              \"num_hidden\": 512,\n",
    "              \"num_hidden_l2\": 256,\n",
    "              \"use_l2\": net_type == \"DSNN\",\n",
    "              \"time_steps\": 10,\n",
    "              \"spike_grad\": spike_grad,\n",
    "              \"beta\": beta,\n",
    "              \"encoding\": 'rate',\n",
    "              \"out_num\": 2\n",
    "              }\n",
    "pop_coding = net_config['out_num'] > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iterations = 30\n",
    "batch_size = 16 #16, 8\n",
    "weight_decay = 0\n",
    "lr = 1e-4\n",
    "optim_type = 'Adam'\n",
    "train_config = {\"num_epochs\": 1000,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['time_steps'],\n",
    "                'val_net': None,\n",
    "                'prediction_fn': get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding),\n",
    "                }\n",
    "drop_last = net_type == \"CSNN\"\n",
    "pin_memory = device == \"cuda\"\n",
    "save = True\n",
    "results = [[], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(metrics_list, all_targets, all_preds):\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    \n",
    "    print(\"acc:\", accuracy,\"auc:\", auc_roc)\n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\\sider\\models\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "net_list = []\n",
    "    \n",
    "net, train_net, val_net, test_net = get_spiking_net(net_type, net_config)\n",
    "filename = make_filename(dirname, target_name, net_type, fp_config, lr, weight_decay, optim_type, net_config, train_config, net, model = True)\n",
    "model_name = filename.removesuffix('.csv')\n",
    "\n",
    "models_path = os.path.join(\"results\", dirname, \"models\", \"\")\n",
    "all_model_names = os.listdir(models_path)\n",
    "print(models_path)\n",
    "#print(all_model_names)\n",
    "for iter in range(iterations):\n",
    "    seed = int(iter + 1)\n",
    "    string_id = f\"seed-{seed}.pth\"\n",
    "    search_name = model_name + str(string_id) \n",
    "    search_name_no_folder = search_name.removeprefix(models_path)\n",
    "    if search_name_no_folder in all_model_names:\n",
    "        state_dict = torch.load(search_name, weights_only=True)\n",
    "        net_list.append(copy.deepcopy(state_dict))\n",
    "    else: print(search_name_no_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:1 -> acc: 0.676056338028169 auc: 0.6750248756218906\n",
      "\n",
      "Seed:2 -> acc: 0.704225352112676 auc: 0.7030241935483871\n",
      "\n",
      "Seed:3 -> acc: 0.7253521126760564 auc: 0.7253521126760563\n",
      "\n",
      "Seed:4 -> acc: 0.7323943661971831 auc: 0.7259701492537314\n",
      "\n",
      "Seed:5 -> acc: 0.7112676056338029 auc: 0.7116339090728608\n",
      "\n",
      "Seed:6 -> acc: 0.6901408450704225 auc: 0.6859241612070678\n",
      "\n",
      "Seed:7 -> acc: 0.7253521126760564 auc: 0.7242063492063492\n",
      "\n",
      "Seed:8 -> acc: 0.6338028169014085 auc: 0.6426829268292682\n",
      "\n",
      "Seed:9 -> acc: 0.6830985915492958 auc: 0.6959117587532888\n",
      "\n",
      "Seed:10 -> acc: 0.6549295774647887 auc: 0.6669651741293533\n",
      "\n",
      "Seed:11 -> acc: 0.7183098591549296 auc: 0.7198009950248755\n",
      "\n",
      "Seed:12 -> acc: 0.7253521126760564 auc: 0.7294657097288676\n",
      "\n",
      "Seed:13 -> acc: 0.6830985915492958 auc: 0.6894715692184047\n",
      "\n",
      "Seed:14 -> acc: 0.7535211267605634 auc: 0.7555092316855271\n",
      "\n",
      "Seed:15 -> acc: 0.6690140845070423 auc: 0.6728468899521531\n",
      "\n",
      "Seed:16 -> acc: 0.6971830985915493 auc: 0.6993243243243243\n",
      "\n",
      "Seed:17 -> acc: 0.704225352112676 auc: 0.7077352472089314\n",
      "\n",
      "Seed:18 -> acc: 0.7112676056338029 auc: 0.7139303482587064\n",
      "\n",
      "Seed:19 -> acc: 0.7183098591549296 auc: 0.7192460317460316\n",
      "\n",
      "Seed:20 -> acc: 0.6830985915492958 auc: 0.6848958333333334\n",
      "\n",
      "Seed:21 -> acc: 0.7394366197183099 auc: 0.7406194163192377\n",
      "\n",
      "Seed:22 -> acc: 0.6619718309859155 auc: 0.6615079365079365\n",
      "\n",
      "Seed:23 -> acc: 0.6690140845070423 auc: 0.6735985533453888\n",
      "\n",
      "Seed:24 -> acc: 0.6971830985915493 auc: 0.6945548489666137\n",
      "\n",
      "Seed:25 -> acc: 0.6830985915492958 auc: 0.6831349206349207\n",
      "\n",
      "Seed:26 -> acc: 0.6690140845070423 auc: 0.6684315684315685\n",
      "\n",
      "Seed:27 -> acc: 0.676056338028169 auc: 0.6925403225806451\n",
      "\n",
      "Seed:28 -> acc: 0.7253521126760564 auc: 0.7322684348000805\n",
      "\n",
      "Seed:29 -> acc: 0.6690140845070423 auc: 0.6848503114325899\n",
      "\n",
      "Seed:30 -> acc: 0.7183098591549296 auc: 0.7331810631229235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iter in range(iterations):\n",
    "    #print(f\"Iteration:{iter + 1}/{iterations}\")\n",
    "    seed = iter + 1\n",
    "    print(f\"Seed:{seed} -> \",end='', flush=True)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # DATA SPLIT\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, fp_config=fp_config, seed=seed, dtype=dtype)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # TESTING\n",
    "    model = net\n",
    "    best_test_auc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    model.load_state_dict(net_list[iter])\n",
    "    model.to(device)\n",
    "    all_preds, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    calc_metrics(results, all_preds=all_preds, all_targets=all_targets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.697 ± 0.028\n",
      "AUC ROC: 0.700 ± 0.026\n",
      "Sensitivity: 0.649 ± 0.067\n",
      "Specificity: 0.752 ± 0.061\n"
     ]
    }
   ],
   "source": [
    "metrics_np = np.zeros(12)\n",
    "\n",
    "for i, metric in enumerate(results):\n",
    "    metrics_np[i*2] = np.round(np.mean(metric), 3)\n",
    "    metrics_np[i*2+1] = np.round(np.std(metric), 3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy:  {metrics_np[0]:.3f} ± {metrics_np[1]:.3f}\")\n",
    "print(f\"AUC ROC: {metrics_np[2]:.3f} ± {metrics_np[3]:.3f}\")\n",
    "print(f\"Sensitivity: {metrics_np[4]:.3f} ± {metrics_np[5]:.3f}\")\n",
    "print(f\"Specificity: {metrics_np[6]:.3f} ± {metrics_np[7]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\\sider\\Hepatobiliary disorders_CSNN_beta-0.95_maccs_morgan_1024_out-8-8_kernel-3_stride-1_t10_e1000_b16_lr0.0001_count_loss_Adam_wd0.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics_np = metrics_np.reshape(1, -1)\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "num_hidden = net_config['num_hidden']\n",
    "time_steps = train_config['num_steps']\n",
    "num_epochs = train_config['num_epochs']\n",
    "\n",
    "\n",
    "filename = make_filename(dirname, target_name, net_type, fp_config, lr, weight_decay, optim_type, net_config, train_config, model)\n",
    "if save: df_metrics.to_csv(filename, index=False)\n",
    "\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
