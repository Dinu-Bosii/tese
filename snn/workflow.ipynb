{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from snn_model import get_loss_fn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from snntorch import spikegen, surrogate\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp_mix, smile_to_fp, data_splitter, get_spiking_net, make_filename\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from csnn_model import CSNNet, get_prediction_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas numpy rdkit torch snntorch matplotlib scikit-learn deepchem pubchempy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/chainer/chainer-chemistry/blob/master/chainer_chemistry/dataset/splitters/scaffold_splitter.py\n",
    "\n",
    "#splitter = ScaffoldSplitter()\n",
    "#tasks, datasets, transformers = dc.molnet.load_bbbp(splitter=splitter, featurizer=\"ECFP\", reload=True)\n",
    "#train, val, test = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hepatobiliary disorders 1427 0.52\n",
      "Metabolism and nutrition disorders 1427 0.7\n",
      "Product issues 1427 0.02\n",
      "Eye disorders 1427 0.61\n",
      "Investigations 1427 0.81\n",
      "Musculoskeletal and connective tissue disorders 1427 0.7\n",
      "Gastrointestinal disorders 1427 0.91\n",
      "Social circumstances 1427 0.18\n",
      "Immune system disorders 1427 0.72\n",
      "Reproductive system and breast disorders 1427 0.51\n",
      "Neoplasms benign, malignant and unspecified (incl cysts and polyps) 1427 0.26\n",
      "General disorders and administration site conditions 1427 0.91\n",
      "Endocrine disorders 1427 0.23\n",
      "Surgical and medical procedures 1427 0.15\n",
      "Vascular disorders 1427 0.78\n",
      "Blood and lymphatic system disorders 1427 0.62\n",
      "Skin and subcutaneous tissue disorders 1427 0.92\n",
      "Congenital, familial and genetic disorders 1427 0.18\n",
      "Infections and infestations 1427 0.7\n",
      "Respiratory, thoracic and mediastinal disorders 1427 0.74\n",
      "Psychiatric disorders 1427 0.71\n",
      "Renal and urinary disorders 1427 0.64\n",
      "Pregnancy, puerperium and perinatal conditions 1427 0.09\n",
      "Ear and labyrinth disorders 1427 0.46\n",
      "Cardiac disorders 1427 0.69\n",
      "Nervous system disorders 1427 0.91\n",
      "Injury, poisoning and procedural complications 1427 0.66\n"
     ]
    }
   ],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[0]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n",
    "\n",
    "for t in targets:\n",
    "    df_temp = df[[t, 'smiles']].dropna()\n",
    "    class_counts = df[t].count()\n",
    "    class_sum = df[t].sum()\n",
    "    print(t, class_counts, round(class_sum/class_counts, 2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "    # SR-MMP\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders 1427 samples, 0.52 class ratio\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILE to Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maccs - 167\n"
     ]
    }
   ],
   "source": [
    "fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['pubchem', 881]]\n",
    "fp_type, num_bits = fp_types[1]\n",
    "#num_bits = 2048\n",
    "fp_config = {\"fp_type\": fp_type,\n",
    "             \"num_bits\": num_bits,\n",
    "             \"radius\": 2,\n",
    "             \"fp_type_2\": fp_types[0][0],\n",
    "             \"num_bits_2\": 1024 - num_bits,\n",
    "             \"mix\": True,\n",
    "             }\n",
    "\n",
    "print(fp_type, '-', num_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:42:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:42:52] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "split = \"scaffold\"\n",
    "dataset = None\n",
    "\n",
    "if dirname != 'BBBP':\n",
    "    split = \"random\"\n",
    "    if fp_config['mix']:\n",
    "        fp_array, target_array = smile_to_fp_mix(df, fp_config=fp_config, target_name=target_name)\n",
    "    else:\n",
    "        fp_array, target_array = smile_to_fp(df, fp_config=fp_config, target_name=target_name)\n",
    "    # Create Torch Dataset\n",
    "    fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "    target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "    dataset = TensorDataset(fp_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1427, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(fp_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_loss\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "loss_types = ['ce_mem', 'rate_loss', 'count_loss', 'temporal_loss', 'bce_loss']\n",
    "loss_type = loss_types[2]\n",
    "print(loss_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_types = [\"SNN\", \"DSNN\", \"CSNN\"]\n",
    "net_type = net_types[2]\n",
    "#spike_grad = surrogate.sigmoid(slope=25)\n",
    "spike_grad = None\n",
    "beta = 0.95 #experimentar 0.7\n",
    "\n",
    "net_config = {\"input_size\": 1024 if fp_config['mix'] else num_bits,\n",
    "              \"num_hidden\": 512,\n",
    "              \"num_hidden_l2\": 256,\n",
    "              \"use_l2\": net_type == \"DSNN\",\n",
    "              \"time_steps\": 10,\n",
    "              \"spike_grad\": spike_grad,\n",
    "              \"beta\": beta,\n",
    "              \"encoding\": 'rate',\n",
    "              \"out_num\": 20\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast sigmoid / sigmoid\n",
    "lr = 1e-5 ou -6, com 500-1000 epochs\n",
    "batch size 16\n",
    "batches devem incluir pos e neg samples\n",
    "Adamax\n",
    "\n",
    "experimentar com Tox21 NR-AR\n",
    "temporal coding se não aumentar com rate coding\n",
    "\n",
    "ver oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "lr=1e-4 #1e-6 default for 1000 epochs. csnn requires higher\n",
    "iterations = 1\n",
    "weight_decay = 0 # 1e-5\n",
    "optim_type = 'Adam'\n",
    "#optim_type = 'SGD'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "batch_size = 16 #16, 8\n",
    "train_config = {\"num_epochs\": 1000,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['time_steps'],\n",
    "                'val_net': None,\n",
    "                'prediction_fn': get_prediction_fn(encoding=net_config['encoding'], pop_coding=net_config['out_num'] > 2),\n",
    "                }\n",
    "drop_last = net_type == \"CSNN\"\n",
    "pin_memory = device == \"cuda\"\n",
    "save = True\n",
    "results = [[], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "\n",
    "# Disable RDKit logging for the scaffold meeting\n",
    "RDLogger.DisableLog('rdApp.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(metrics_list, all_targets, all_preds):\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    \n",
    "    print(accuracy, auc_roc)\n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:1/1\n",
      "Seed:26\n",
      "512\n",
      "Epoch:10|auc:0.626078431372549|loss:0.7167485952377319\n",
      "Epoch:20|auc:0.686078431372549|loss:0.23657259345054626\n",
      "Epoch:30|auc:0.6566666666666666|loss:0.1120108962059021\n",
      "Epoch:40|auc:0.6086274509803921|loss:0.2519812285900116\n",
      "Epoch:50|auc:0.6447058823529411|loss:0.10038323700428009\n",
      "Epoch:60|auc:0.6288235294117648|loss:0.04902616888284683\n",
      "Epoch:70|auc:0.6354901960784314|loss:0.05040775239467621\n",
      "Epoch:80|auc:0.627450980392157|loss:0.0398968830704689\n",
      "Epoch:90|auc:0.6754901960784314|loss:0.004697759170085192\n",
      "Epoch:100|auc:0.6148039215686275|loss:0.028551559895277023\n",
      "Epoch:110|auc:0.590686274509804|loss:0.05307954549789429\n",
      "Epoch:120|auc:0.6086274509803921|loss:0.026721693575382233\n",
      "Epoch:130|auc:0.6200980392156863|loss:0.0026993327774107456\n",
      "Epoch:140|auc:0.6095098039215686|loss:0.0042657870799303055\n",
      "Epoch:150|auc:0.6442156862745098|loss:0.004268300719559193\n",
      "Epoch:160|auc:0.5665686274509804|loss:0.14016617834568024\n",
      "Epoch:170|auc:0.6207843137254903|loss:0.0006118293385952711\n",
      "Epoch:180|auc:0.5987254901960785|loss:0.0010925083188340068\n",
      "Epoch:190|auc:0.574607843137255|loss:0.001930080121383071\n",
      "Epoch:200|auc:0.5973529411764705|loss:0.00033191387774422765\n",
      "Epoch:210|auc:0.6120588235294117|loss:0.00016171508468687534\n",
      "Epoch:220|auc:0.5826470588235294|loss:0.00022026557417120785\n",
      "Epoch:230|auc:0.6047058823529411|loss:0.00013749435311183333\n",
      "Epoch:240|auc:0.5853921568627451|loss:0.010859273374080658\n",
      "Epoch:250|auc:0.6187254901960784|loss:0.000384423416107893\n",
      "Epoch:260|auc:0.6060784313725489|loss:0.0002892533375415951\n",
      "Epoch:270|auc:0.6253921568627451|loss:0.00010327101335860789\n",
      "Epoch:280|auc:0.59|loss:0.00018701930821407586\n",
      "Epoch:290|auc:0.6040196078431372|loss:0.00016486523963976651\n",
      "Epoch:300|auc:0.5886274509803922|loss:7.065750833135098e-05\n",
      "Epoch:310|auc:0.638235294117647|loss:0.012866204604506493\n",
      "Epoch:320|auc:0.6334313725490196|loss:0.00018360590911470354\n",
      "Epoch:330|auc:0.6394117647058823|loss:0.0001650557096581906\n",
      "Epoch:340|auc:0.6194117647058823|loss:7.410364923998713e-05\n",
      "Epoch:350|auc:0.6173529411764705|loss:0.0001799846941139549\n",
      "Epoch:360|auc:0.6060784313725489|loss:8.630118099972606e-05\n",
      "Epoch:370|auc:0.6180392156862745|loss:0.00011343664664309472\n",
      "Epoch:380|auc:0.6127450980392157|loss:5.1915194489993155e-05\n",
      "Epoch:390|auc:0.6012745098039215|loss:9.946619684342295e-06\n",
      "Epoch:400|auc:0.5853921568627451|loss:0.00020888616563752294\n",
      "Epoch:410|auc:0.5847058823529412|loss:0.00011822408851003274\n",
      "Epoch:420|auc:0.5773529411764705|loss:0.00014548587205354124\n",
      "Epoch:430|auc:0.5780392156862745|loss:0.0001402240595780313\n",
      "Epoch:440|auc:0.6026470588235294|loss:0.00016642577247694135\n",
      "Epoch:450|auc:0.6113725490196078|loss:4.3930005631409585e-05\n",
      "Epoch:460|auc:0.5994117647058823|loss:1.3086151739116758e-05\n",
      "Epoch:470|auc:0.5752941176470588|loss:1.2637525287573226e-05\n",
      "Epoch:480|auc:0.5994117647058823|loss:1.3144000149623025e-05\n",
      "Epoch:490|auc:0.5980392156862746|loss:4.513865860644728e-05\n",
      "Epoch:500|auc:0.6134313725490196|loss:0.0001325721968896687\n",
      "Epoch:510|auc:0.606764705882353|loss:9.715260239318013e-05\n",
      "Epoch:520|auc:0.5934313725490196|loss:9.467401105212048e-05\n",
      "Epoch:530|auc:0.5913725490196078|loss:2.903488712036051e-05\n",
      "Epoch:540|auc:0.5913725490196078|loss:2.499563379387837e-05\n",
      "Epoch:550|auc:0.5973529411764705|loss:0.00010599495726637542\n",
      "Epoch:560|auc:0.5987254901960785|loss:2.1238338376861066e-05\n",
      "Epoch:570|auc:0.5826470588235294|loss:0.00018777302466332912\n",
      "Epoch:580|auc:0.6141176470588235|loss:3.151426062686369e-05\n",
      "Epoch:590|auc:0.5819607843137256|loss:5.098333349451423e-05\n",
      "Epoch:600|auc:0.6200980392156863|loss:0.0015427782200276852\n",
      "Epoch:610|auc:0.6127450980392157|loss:0.00020217575365677476\n",
      "Epoch:620|auc:0.6194117647058823|loss:6.354447396006435e-05\n",
      "Epoch:630|auc:0.6134313725490196|loss:9.94010224530939e-06\n",
      "Epoch:640|auc:0.6127450980392157|loss:4.383788473205641e-05\n",
      "Epoch:650|auc:0.6267647058823529|loss:3.97527874156367e-05\n",
      "Epoch:660|auc:0.606764705882353|loss:5.428577424027026e-05\n",
      "Epoch:670|auc:0.6053921568627452|loss:1.417826297256397e-05\n",
      "Epoch:680|auc:0.607450980392157|loss:2.772200787148904e-05\n",
      "Epoch:690|auc:0.5973529411764705|loss:1.3528418776331819e-06\n",
      "Epoch:700|auc:0.6040196078431372|loss:1.8637831090018153e-05\n",
      "Epoch:710|auc:0.6152941176470589|loss:4.245573654770851e-05\n",
      "Epoch:720|auc:0.6267647058823529|loss:0.000219685971387662\n",
      "Epoch:730|auc:0.6127450980392157|loss:3.253515387768857e-05\n",
      "Epoch:740|auc:0.6200980392156863|loss:0.00010275837121298537\n",
      "Epoch:750|auc:0.6134313725490196|loss:1.767000139807351e-05\n",
      "Epoch:760|auc:0.606764705882353|loss:3.2305048080161214e-05\n",
      "Epoch:770|auc:0.6053921568627452|loss:4.968843313690741e-06\n",
      "Epoch:780|auc:0.6194117647058823|loss:2.030068208114244e-05\n",
      "Epoch:790|auc:0.6040196078431372|loss:5.905661600991152e-05\n",
      "Epoch:800|auc:0.6047058823529411|loss:4.90907450512168e-06\n",
      "Epoch:810|auc:0.6141176470588235|loss:9.287408829550259e-06\n",
      "Epoch:820|auc:0.6141176470588235|loss:1.1386280675651506e-05\n",
      "Epoch:830|auc:0.6106862745098038|loss:0.0005764797097072005\n",
      "Epoch:840|auc:0.6194117647058823|loss:0.00017287417722400278\n",
      "Epoch:850|auc:0.6194117647058823|loss:4.2970241338480264e-05\n",
      "Epoch:860|auc:0.6194117647058823|loss:4.313460158300586e-05\n",
      "Epoch:870|auc:0.6267647058823529|loss:1.3017693163419608e-05\n",
      "Epoch:880|auc:0.6267647058823529|loss:2.6120902475668117e-05\n",
      "Epoch:890|auc:0.6120588235294117|loss:1.6035826774896123e-05\n",
      "Epoch:900|auc:0.6267647058823529|loss:2.3881329980213195e-05\n",
      "Epoch:910|auc:0.6060784313725489|loss:3.444331741775386e-05\n",
      "Epoch:920|auc:0.6127450980392157|loss:1.0263540389132686e-05\n",
      "Epoch:930|auc:0.6200980392156863|loss:6.813384629822394e-07\n",
      "Epoch:940|auc:0.6141176470588235|loss:5.483061613631435e-06\n",
      "Epoch:950|auc:0.6127450980392157|loss:7.1587637648917735e-06\n",
      "Epoch:960|auc:0.6134313725490196|loss:6.606306851608679e-05\n",
      "Epoch:970|auc:0.6134313725490196|loss:2.7585878342506476e-05\n",
      "Epoch:980|auc:0.5987254901960785|loss:1.4671665667265188e-05\n",
      "Epoch:990|auc:0.5987254901960785|loss:1.3511371435015462e-05\n",
      "Epoch:1000|auc:0.6060784313725489|loss:1.187269026559079e-05\n",
      "616 0.6982017982017982\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), filename\u001b[38;5;241m.\u001b[39mremovesuffix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m all_preds, all_targets \u001b[38;5;241m=\u001b[39m test_net(model, device, test_loader, train_config)\n\u001b[1;32m---> 58\u001b[0m \u001b[43mcalc_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_targets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m, in \u001b[0;36mcalc_metrics\u001b[1;34m(metrics_list, all_targets, all_preds)\u001b[0m\n\u001b[0;32m      3\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(all_targets, all_preds)\n\u001b[0;32m      4\u001b[0m auc_roc \u001b[38;5;241m=\u001b[39m roc_auc_score(all_targets, all_preds)\n\u001b[1;32m----> 5\u001b[0m tn, fp, fn, tp \u001b[38;5;241m=\u001b[39m confusion_matrix(all_targets, all_preds)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m      6\u001b[0m sensitivity \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m/\u001b[39m(tp \u001b[38;5;241m+\u001b[39m fn)\n\u001b[0;32m      7\u001b[0m specificity \u001b[38;5;241m=\u001b[39m tn\u001b[38;5;241m/\u001b[39m(tn \u001b[38;5;241m+\u001b[39m fp)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "for iter in range(iterations):\n",
    "    print(f\"Iteration:{iter + 1}/{iterations}\")\n",
    "    seed = random.randint(1, 30)\n",
    "    # seed = iter + 1\n",
    "    print(f\"Seed:{seed}\")\n",
    "    random.seed(seed)\n",
    "\n",
    "    net, train_net, val_net, test_net = get_spiking_net(net_type, net_config)\n",
    "    net = net.to(device)\n",
    "    train_config['val_net'] = val_net\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    #optimizer = torch.optim.AdamW(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    #optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_config['num_epochs'])\n",
    "    #optimizer = torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    #train_config[\"scheduler\"] = scheduler\n",
    "\n",
    "    # DATA SPLIT\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, fp_config=fp_config, seed=seed, dtype=dtype)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # LOSS FN\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1], dtype=np.int8), y=np.array(train_label, dtype=np.int8))\n",
    "    #class_weights[0] = class_weights[0]/2 \n",
    "    #class_weights[0] = class_weights[0]*2\n",
    "\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "\n",
    "    train_config[\"loss_fn\"] = get_loss_fn(loss_type=loss_type, class_weights=class_weights)\n",
    "\n",
    "\n",
    "    # TRAINING\n",
    "    net, loss_hist, val_acc_hist, val_auc_hist, net_list = train_net(net=net, optimizer=optimizer, train_loader=train_loader, val_loader=val_loader, train_config=train_config, net_config=net_config)\n",
    "    \n",
    "    # TESTING\n",
    "    model = net\n",
    "    best_test_auc = 0\n",
    "    best_epoch = 0\n",
    "    for index, model_dict in enumerate(net_list):\n",
    "        model.load_state_dict(model_dict)\n",
    "        model.to(device)\n",
    "        all_preds2, all_targets2 = test_net(model, device, test_loader, train_config)\n",
    "        auc_roc_test = roc_auc_score(all_targets2, all_preds2)\n",
    "        if auc_roc_test > best_test_auc:\n",
    "            best_test_auc, best_epoch = (auc_roc_test, index)\n",
    "\n",
    "    print(best_epoch, best_test_auc)\n",
    "    model.load_state_dict(net_list[best_epoch])\n",
    "    filename = make_filename(dirname, target_name, net_type, fp_config, lr, weight_decay, optim_type, net_config, train_config, model)\n",
    "    torch.save(model.state_dict(), filename.removesuffix('csv') + 'pth')\n",
    "    all_preds, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    calc_metrics(results, all_preds=all_preds, all_targets=all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the best model\n",
    "#torch.save(model.state_dict(), models//)\n",
    "# se for preciso, continuar o treino\n",
    "print('best val set auc:', max(val_auc_hist))\n",
    "print(model.lif1.threshold)\n",
    "print(model.lif_out.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630 0.6696303696303697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = net\n",
    "best_test_auc = 0\n",
    "best_epoch = 0\n",
    "for index, model_dict in enumerate(net_list):\n",
    "    model.load_state_dict(model_dict)\n",
    "    model.to(device)\n",
    "    _, auc_roc_test = val_net(model, device, test_loader, train_config)\n",
    "\n",
    "    if auc_roc_test > best_test_auc:\n",
    "        best_test_auc, best_epoch = (auc_roc_test, index)\n",
    "\n",
    "print(best_epoch, best_test_auc)\n",
    "model.load_state_dict(net_list[best_epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothed Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "#from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "#print(loss_hist[len(loss_hist) - 5:len(loss_hist)])\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "#plt.plot(np.convolve(loss_hist, np.ones(30)/30, mode='valid'))\n",
    "#plt.plot(savgol_filter(loss_hist, window_length=100, polyorder=3))\n",
    "#plt.plot(lowess(loss_hist, np.arange(len(loss_hist)), frac=0.1)[:, 1])\n",
    "plt.plot(gaussian_filter1d(loss_hist, sigma=6))\n",
    "#plt.plot(loss_hist)\n",
    "#plt.axhline(y=1, color='r', linestyle='--', label='y = 1')\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = train_config['num_epochs']\n",
    "num_minibatches_per_epoch = len(loss_hist) // num_epochs\n",
    "\n",
    "# Create x-axis values in terms of epochs\n",
    "epochs = np.linspace(1, num_epochs, len(loss_hist))\n",
    "epoch_losses = np.array(loss_hist).reshape(num_epochs, num_minibatches_per_epoch).mean(axis=1)\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label=\"Loss per Epoch\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Set\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "\n",
    "#plt.plot(gaussian_filter1d(val_auc_hist, sigma=6))\n",
    "plt.plot(val_auc_hist)\n",
    "plt.title(\"ROC AUC on Validation Set\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_np = np.zeros(12)\n",
    "\n",
    "for i, metric in enumerate(results):\n",
    "    metrics_np[i*2] = np.round(np.mean(metric), 3)\n",
    "    metrics_np[i*2+1] = np.round(np.std(metric), 3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy:  {metrics_np[0]:.3f} ± {metrics_np[1]:.3f}\")\n",
    "print(f\"AUC ROC: {metrics_np[2]:.3f} ± {metrics_np[3]:.3f}\")\n",
    "print(f\"Sensitivity: {metrics_np[4]:.3f} ± {metrics_np[5]:.3f}\")\n",
    "print(f\"Specificity: {metrics_np[6]:.3f} ± {metrics_np[7]:.3f}\")\n",
    "\n",
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics_np = metrics_np.reshape(1, -1)\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "num_hidden = net_config['num_hidden']\n",
    "time_steps = train_config['num_steps']\n",
    "num_epochs = train_config['num_epochs']\n",
    "\n",
    "\n",
    "filename = make_filename(dirname, target_name, net_type, fp_config, lr, weight_decay, optim_type, net_config, train_config, model)\n",
    "if save: df_metrics.to_csv(filename, index=False)\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "#summary(net, input_size=(1, 1024),  batch_size=32)\n",
    "\n",
    "#from torchinfo import summary\n",
    "\n",
    "#summary(net, input_size=(batch_size, 1, 1024), verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
