{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp,smiles_to_descriptor,smiles_to_onehot, smiles_to_onehot_selfies, data_splitter\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = f\"./results/logs/output_{timestamp}.txt\"\n",
    "log_file = open(log_path, \"w\")\n",
    "sys.stdout = log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[1]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n",
    "print(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "elif dirname == 'sider':\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()\n",
    "\n",
    "print(target_name)\n",
    "print(df[target_name].sum())\n",
    "print(df[target_name].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Molecular Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILE to Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"fp\", \"descriptor\", \"SELFIES-1hot\", \"SMILES-1hot\"]#, \"graph-list\"]\n",
    "\n",
    "repr_type = representations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repr_type == \"fp\":\n",
    "    fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['count_morgan', 1024], ['pubchem', 881]]\n",
    "    mix = False\n",
    "    fp_type, num_bits = fp_types[2]\n",
    "    if mix and fp_type == 'RDKit':\n",
    "        num_bits = 512\n",
    "    data_config = {\"fp_type\": fp_type,\n",
    "                \"num_bits\": num_bits,\n",
    "                \"radius\": 2,\n",
    "                \"fp_type_2\": fp_types[0][0],\n",
    "                \"num_bits_2\": 1024 - num_bits,\n",
    "                \"mix\": mix,}\n",
    "    dim_2 = False\n",
    "    print(fp_type, '-', num_bits)\n",
    "    if mix: print(data_config['fp_type_2'], '-', data_config['num_bits_2'])\n",
    "    if dim_2: print(\"2D FP\")\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    desc_type = [\"RDKit\", \"TODO\"]\n",
    "    data_config = {\"desc\": desc_type[0],\n",
    "                   \"num_bits\": 0,\n",
    "                }\n",
    "elif repr_type == \"SELFIES-1hot\":\n",
    "    dim_2 = True\n",
    "    data_config = {}\n",
    "\n",
    "elif repr_type == \"SMILES-1hot\":\n",
    "    dim_2 = True\n",
    "    data_config = {}\n",
    "\n",
    "data_config[\"repr_type\"] = repr_type\n",
    "print(repr_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:04:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:38] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:38] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:38] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:38] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:04:55] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "split = \"scaffold\"\n",
    "dataset = None\n",
    "\n",
    "if dirname != 'BBBP':\n",
    "    split = \"random\"\n",
    "    if repr_type == \"fp\":\n",
    "        fp_array, target_array = smile_to_fp(df, data_config=data_config, target_name=target_name)\n",
    "        # Create Torch Dataset\n",
    "        fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "        print(fp_tensor.size())\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "        if dim_2:\n",
    "            fp_tensor = fp_tensor.view(-1, 32, 32)\n",
    "            print(fp_tensor.size())\n",
    "        dataset = TensorDataset(fp_tensor, target_tensor)\n",
    "    elif repr_type == \"descriptor\":\n",
    "        desc_array, target_array = smiles_to_descriptor(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        desc_tensor = torch.tensor(desc_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(desc_tensor, target_tensor)\n",
    "        print(desc_tensor.size())\n",
    "    elif repr_type == \"SELFIES-1hot\":\n",
    "        selfies_array, target_array = smiles_to_onehot_selfies(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        selfies_tensor = torch.tensor(selfies_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(selfies_tensor, target_tensor)\n",
    "        print(selfies_tensor.size())\n",
    "    elif repr_type == \"SMILES-1hot\":\n",
    "        smiles_array, target_array = smiles_to_onehot(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        smiles_tensor = torch.tensor(smiles_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(smiles_tensor, target_tensor)\n",
    "        print(smiles_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repr_type == \"fp\":\n",
    "    data_config[\"input_size\"] = 1024 if data_config['mix'] else num_bits\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    data_config[\"input_size\"] = desc_tensor.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics ---- acc  roc sn  sp  f1  prs\n",
    "svm_metrics = [[], [], [], [], [], []]\n",
    "rf_metrics  = [[], [], [], [], [], []]\n",
    "knn_metrics = [[], [], [], [], [], []]\n",
    "xgb_metrics = [[], [], [], [], [], []]\n",
    "mlp_metrics = [[], [], [], [], [], []]\n",
    "metrics = [svm_metrics, rf_metrics, xgb_metrics, knn_metrics, mlp_metrics]\n",
    "\n",
    "grid_parameters = {\n",
    "    \"SVM\": {\n",
    "        \"C\": list(range(1, 100)),\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"],\n",
    "        \"degree\": [2, 3, 4],\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"max_depth\": [5] + list(range(10, 100, 10)),\n",
    "        \"n_estimators\": list(range(50, 400, 50)),\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"learning_rate\": [0.005, 0.01, 0.1, 0.2],\n",
    "        \"max_depth\": range(2, 20, 2),\n",
    "        \"n_estimators\": range(50, 400, 50),\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": list(range(1, 20)),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "    },\n",
    "    \"MLP\": {\n",
    "        'hidden_layer_sizes': [(512,), (1024,), (512,256), (1024,512), (1024, 256)],\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0, 0.0001, 0.001],   # L2 regularization\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        \"early_stopping\": [True],\n",
    "        \"max_iter\": [1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "knn_best_params = []\n",
    "svm_best_params = []\n",
    "rf_best_params = []\n",
    "xgb_best_params = []\n",
    "mlp_best_params = []\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(metrics_list, y_pred, y_true):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_norm(train_subset, val_subset, test_subset):\n",
    "    train_tensor, _ = train_subset[:]\n",
    "    val_tensor, _ = val_subset[:]\n",
    "    test_tensor, _ = test_subset[:]\n",
    "\n",
    "    min_val = train_tensor.min(dim=0).values\n",
    "    max_val = train_tensor.max(dim=0).values\n",
    "    range_val = (max_val - min_val).clamp(min=1e-6)\n",
    "\n",
    "    train_norm = ((train_tensor - min_val) / range_val).clamp(0.0, 1.0)\n",
    "    val_norm   = ((val_tensor   - min_val) / range_val).clamp(0.0, 1.0)\n",
    "    test_norm  = ((test_tensor  - min_val) / range_val).clamp(0.0, 1.0)\n",
    "\n",
    "    return train_norm, val_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, train_X, train_Y, test_X, test_Y, metrics_list):\n",
    "    if isinstance(model, MLPClassifier):\n",
    "        sample_weights = compute_sample_weight(class_weight='balanced', y=train_Y)\n",
    "        model.fit(train_X, train_Y, sample_weight=sample_weights)\n",
    "    else:\n",
    "        model.fit(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    calculate_metrics(metrics_list=metrics_list, y_true=test_Y, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_param_search(model, grid_param, train_X, train_Y):\n",
    "    search = RandomizedSearchCV(model, grid_param, n_iter=40, scoring='roc_auc', random_state=42, verbose=1)\n",
    "    if isinstance(model, MLPClassifier):\n",
    "        sample_weights = compute_sample_weight(class_weight='balanced', y=train_Y)\n",
    "        search.fit(train_X, train_Y, sample_weight=sample_weights)\n",
    "    else:\n",
    "        search.fit(train_X, train_Y)\n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_weight = (sum(train_Y == 1) / sum(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 30 is smaller than n_iter=40. Running 30 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"C:\\Users\\knsve\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\subprocess.py\", line 501, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"C:\\Users\\knsve\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\subprocess.py\", line 966, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\knsve\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\subprocess.py\", line 1435, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    }
   ],
   "source": [
    "iterations = 30\n",
    "print(\"Iterations:\")\n",
    "for iter in range(iterations):\n",
    "    print(str(iter) + \"/30\", flush=True)\n",
    "    seed = iter+1\n",
    "    random.seed(seed)\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, data_config=data_config, seed=seed, dtype=dtype)\n",
    "    train_X, train_Y = train[:]\n",
    "    val_X, val_Y = val[:]\n",
    "    test_X, test_Y = test[:]\n",
    "    if repr_type == \"descriptor\":\n",
    "        train_X, val_X, test_X = minmax_norm(train, val, test)\n",
    "\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.array(train_Y))\n",
    "    #class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    class_weights_dict = \"balanced\"\n",
    "\n",
    "    pos_weight = (sum(train_Y == 1).numpy() / sum(train_Y == 0).numpy())\n",
    "\n",
    "\n",
    "    if iter == 0:\n",
    "        print(\"MLP parameter search...\", flush=True)\n",
    "        MLP =  MLPClassifier()\n",
    "        mlp_best_params = random_param_search(MLP, grid_parameters['MLP'], train_X, train_Y)\n",
    "        print(\"XGBoost parameter search...\", flush=True)\n",
    "        XGB = XGBClassifier(objective=\"binary:logistic\", scale_pos_weight=pos_weight)\n",
    "        xgb_best_params = random_param_search(XGB, grid_parameters['XGB'], train_X, train_Y)\n",
    "        print(\"KNN parameter search...\", flush=True)\n",
    "        KNN = KNeighborsClassifier()\n",
    "        knn_best_params = random_param_search(KNN, grid_parameters['KNN'], train_X, train_Y)\n",
    "        print(\"SVM parameter search...\", flush=True)\n",
    "        SVM = svm.SVC(class_weight=class_weights_dict, random_state=seed, probability=False)\n",
    "        svm_best_params = random_param_search(SVM, grid_parameters['SVM'], train_X, train_Y)\n",
    "        print(\"Random Forest parameter search...\", flush=True)\n",
    "        RF = RandomForestClassifier(class_weight=class_weights_dict, random_state=seed)\n",
    "        rf_best_params = random_param_search(RF, grid_parameters['RF'], train_X, train_Y)\n",
    "\n",
    "        print(knn_best_params, svm_best_params, rf_best_params, xgb_best_params)\n",
    "\n",
    "    SVM = svm.SVC(**svm_best_params, class_weight=class_weights_dict, random_state=seed, probability=False)\n",
    "    RF = RandomForestClassifier(**rf_best_params, class_weight=class_weights_dict, random_state=seed)\n",
    "    XGB = XGBClassifier(**xgb_best_params, objective=\"binary:logistic\", scale_pos_weight=pos_weight, random_state=seed)\n",
    "    KNN = KNeighborsClassifier(**knn_best_params)\n",
    "    MLP =  MLPClassifier(**mlp_best_params, random_state=seed)\n",
    "\n",
    "    models = [SVM, RF, XGB, KNN, MLP]\n",
    "    for i, model in enumerate(models):\n",
    "        train_test_model(model, train_X, train_Y, test_X, test_Y, metrics[i])\n",
    "    for m in metrics:\n",
    "        print(m[1][iter], flush=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics = [svm_metrics, rf_metrics, xgb_metrics, knn_metrics, mlp_metrics]\n",
    "metrics_np = np.zeros((len(metrics), 12))\n",
    "\n",
    "for i, clf in enumerate(metrics):\n",
    "    metrics_np[i, 0::2] = np.round([np.mean(metric) for metric in clf], 3)\n",
    "    metrics_np[i, 1::2] = np.round([np.std(metric) for metric in clf], 3)  \n",
    "\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "print(metrics_np)\n",
    "clfs = [\"SVM\", \"RF\",\"XGB\", \"KNN\", \"MLP\"]\n",
    "df_clfs = pd.DataFrame(clfs, columns=[\"Classifier\"])\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "df = pd.concat([df_clfs, df_metrics], axis=1)\n",
    "blank = pd.DataFrame([[\"\"] * 12] * 3, columns=columns) \n",
    "\n",
    "if repr_type == \"fp\":\n",
    "    if data_config['mix']:\n",
    "        filename = f\"results\\\\{dirname}\\\\ml_{fp_type}_{data_config['fp_type_2']}_{target_name}.csv\"\n",
    "\n",
    "    elif fp_type in ['maccs', 'pubchem']:\n",
    "        filename = f\"results\\\\{dirname}\\\\ml_{fp_type}_{target_name}.csv\"\n",
    "    else:\n",
    "        filename = f\"results\\\\{dirname}\\\\ml_{fp_type}_{num_bits}_{target_name}.csv\"\n",
    "elif repr_type == \"descriptor\":\n",
    "    filename = f\"results\\\\{dirname}\\\\ml_desc_217_{target_name}.csv\"\n",
    "\n",
    "filename = filename.replace(\" \", \"_\")\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "for i in range(len(metrics)):\n",
    "    df_raw = pd.DataFrame({name: metrics[i][j] for j, name in enumerate(metric_names)})\n",
    "    df_raw[\"Seed\"] = list(range(1, 31))\n",
    "    df_raw = df_raw[[\"Seed\"] + metric_names]  # reorder columns\n",
    "    blank.to_csv(filename, mode='a', index=False, header=False)\n",
    "    pd.DataFrame([[\"Classifier: \" + clfs[i]]], columns=[df_raw.columns[0]]).to_csv(filename, mode='a', index=False, header=False)\n",
    "    df_raw.to_csv(filename, mode='a', index=False)\n",
    "\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
