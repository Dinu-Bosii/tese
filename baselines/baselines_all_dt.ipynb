{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp, data_splitter\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p_np']\n"
     ]
    }
   ],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[1]\n",
    "dirname = dt_file.strip('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n",
    "print(targets)\n",
    "\n",
    "target_name = targets[0]\n",
    "df = df[[target_name, 'smiles']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_np\n",
      "1567\n",
      "2050\n"
     ]
    }
   ],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "elif dirname == 'sider':\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()\n",
    "\n",
    "print(target_name)\n",
    "print(df[target_name].sum())\n",
    "print(df[target_name].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILE to Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maccs - 167\n"
     ]
    }
   ],
   "source": [
    "fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['pubchem', 881]]\n",
    "fp_type, num_bits = fp_types[1]\n",
    "#num_bits = 2048\n",
    "fp_config = {\"fp_type\": fp_type,\n",
    "             \"num_bits\": num_bits}\n",
    "#num_bits = 237\n",
    "print(fp_type, '-', num_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "split = \"scaffold\"\n",
    "dataset = None\n",
    "if dirname != 'BBBP':\n",
    "    split = \"random\"\n",
    "    fp_array, target_array = smile_to_fp(df, fp_config=fp_config, target_name=target_name)\n",
    "    # Create Torch Dataset\n",
    "    fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "    target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "    dataset = TensorDataset(fp_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics ---- roc  acc f1  prs sns sps\n",
    "svm_metrics = [[], [], [], [], [], []]\n",
    "rf_metrics  = [[], [], [], [], [], []]\n",
    "knn_metrics = [[], [], [], [], [], []]\n",
    "xgb_metrics = [[], [], [], [], [], []]\n",
    "mlp_metrics = [[], [], [], [], [], []]\n",
    "metrics = [svm_metrics, rf_metrics, xgb_metrics, knn_metrics, mlp_metrics]\n",
    "\n",
    "grid_parameters = {\n",
    "    \"SVM\": {\n",
    "        \"C\": list(range(1, 100)),\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"],\n",
    "        \"degree\": [2, 3, 4],\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"max_depth\": [5] + list(range(10, 100, 10)),\n",
    "        \"n_estimators\": list(range(50, 400, 50)),\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"learning_rate\": [0.005, 0.01, 0.1, 0.2],\n",
    "        \"max_depth\": range(2, 20, 2),\n",
    "        \"n_estimators\": range(50, 400, 50),\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": list(range(1, 20)),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "knn_best_params = []\n",
    "svm_best_params = []\n",
    "rf_best_params = []\n",
    "xgb_best_params = []\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(metrics_list, y_pred, y_true):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, train_X, train_Y, test_X, test_Y, metrics_list):\n",
    "    model.fit(train_X,train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    calculate_metrics(metrics_list=metrics_list, y_true=test_Y, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_param_search(model, grid_param, train_X, train_Y):\n",
    "    search = RandomizedSearchCV(model, grid_param, n_iter=40, scoring='roc_auc', random_state=42)\n",
    "    search.fit(train_X, train_Y)\n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_weight = (sum(train_Y == 1) / sum(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      "0/30\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m----> 7\u001b[0m train, val, test \u001b[38;5;241m=\u001b[39m data_splitter(\u001b[43mdf\u001b[49m, target_name, split\u001b[38;5;241m=\u001b[39msplit, dataset\u001b[38;5;241m=\u001b[39mdataset, fp_config\u001b[38;5;241m=\u001b[39mfp_config, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m      8\u001b[0m train_X, train_Y \u001b[38;5;241m=\u001b[39m train[:]\n\u001b[0;32m      9\u001b[0m val_X, val_Y \u001b[38;5;241m=\u001b[39m val[:]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "iterations = 30\n",
    "print(\"Iterations:\")\n",
    "for iter in range(iterations):\n",
    "    print(str(iter) + \"/30\")\n",
    "    seed = iter+1\n",
    "    random.seed(seed)\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, fp_config=fp_config, seed=iter+1, dtype=dtype)\n",
    "    train_X, train_Y = train[:]\n",
    "    val_X, val_Y = val[:]\n",
    "    test_X, test_Y = test[:]\n",
    "    \n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.array(train_Y))\n",
    "    #class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    class_weights_dict = \"balanced\"\n",
    "\n",
    "    pos_weight = (sum(train_Y == 1).numpy() / sum(train_Y == 0).numpy())\n",
    "\n",
    "\n",
    "    if iter == 0:\n",
    "        print(\"KNN parameter search...\")\n",
    "        KNN = KNeighborsClassifier()\n",
    "        knn_best_params = random_param_search(KNN, grid_parameters['KNN'], train_X, train_Y)\n",
    "        print(\"SVM parameter search...\")\n",
    "        SVM = svm.SVC(class_weight=class_weights_dict, random_state=seed)\n",
    "        svm_best_params = random_param_search(SVM, grid_parameters['SVM'], train_X, train_Y)\n",
    "        print(\"Random Forest parameter search...\")\n",
    "        RF = RandomForestClassifier(class_weight=class_weights_dict, random_state=seed)\n",
    "        rf_best_params = random_param_search(RF, grid_parameters['RF'], train_X, train_Y)\n",
    "        print(\"XGBoost parameter search...\")\n",
    "        XGB = XGBClassifier(objective=\"binary:logistic\", scale_pos_weight=pos_weight)\n",
    "        xgb_best_params = random_param_search(XGB, grid_parameters['XGB'], train_X, train_Y)\n",
    "\n",
    "        print(knn_best_params, svm_best_params, rf_best_params, xgb_best_params)\n",
    "\n",
    "    SVM = svm.SVC(**svm_best_params, class_weight=class_weights_dict, random_state=seed)\n",
    "    RF = RandomForestClassifier(**rf_best_params, class_weight=class_weights_dict, random_state=seed)\n",
    "    XGB = XGBClassifier(**xgb_best_params, objective=\"binary:logistic\", scale_pos_weight=pos_weight, random_state=seed)\n",
    "    KNN = KNeighborsClassifier(**knn_best_params)\n",
    "    MLP =  MLPClassifier(hidden_layer_sizes=(num_bits), activation='relu', solver='adam', max_iter=1000)\n",
    "\n",
    "    models = [SVM, RF, XGB, KNN, MLP]\n",
    "    for i, model in enumerate(models):\n",
    "        train_test_model(model, train_X, train_Y, test_X, test_Y, metrics[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterations = 30\n",
    "print(\"Iterations:\")\n",
    "for iter in range(iterations):\n",
    "    print(str(iter) + \"/30\")\n",
    "    seed = iter+1\n",
    "    random.seed(seed)\n",
    "    \n",
    "    #train_X, test_X, train_Y, test_Y = model_selection.train_test_split(fp_array,target_array, test_size=0.3, shuffle=True, random_state=seed)\n",
    "    \n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, fp_config=fp_config, seed=iter+1, dtype=dtype)\n",
    "    train_X, train_Y = train[:]\n",
    "    val_X, val_Y = val[:]\n",
    "    test_X, test_Y = test[:]\n",
    "    \n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.array(train_Y))\n",
    "    #class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    #class_weights_dict = \"balanced\"\n",
    "\n",
    "\n",
    "    #################### SVM ####################\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', class_weight=class_weights_dict, random_state=seed)\n",
    "\n",
    "    SVM.fit(train_X,train_Y)\n",
    "\n",
    "    predictions_SVM = SVM.predict(test_X)\n",
    "    \n",
    "    calculate_metrics(metrics_list=svm_metrics, y_true=test_Y, y_pred=predictions_SVM)\n",
    "\n",
    "    #################### RF ####################\n",
    "\n",
    "    RF = RandomForestClassifier(max_depth=10, n_estimators=100, class_weight=class_weights_dict, random_state=seed)\n",
    "\n",
    "    RF.fit(train_X,train_Y)\n",
    "\n",
    "    predictions_RF = RF.predict(test_X)\n",
    "\n",
    "    calculate_metrics(metrics_list=rf_metrics, y_true=test_Y, y_pred=predictions_RF)\n",
    "\n",
    "    #################### XGB ####################\n",
    "    #pos_weight = sum(train_Y == 0) / sum(train_Y == 1)\n",
    "    pos_weight=1\n",
    "    XGB = XGBClassifier(objective=\"binary:logistic\",learning_rate=0.1,max_depth=6,n_estimators=100,scale_pos_weight=pos_weight)\n",
    "\n",
    "    XGB.fit(train_X,train_Y)\n",
    "\n",
    "    predictions_XGB = XGB.predict(test_X)\n",
    "\n",
    "    calculate_metrics(metrics_list=xgb_metrics, y_true=test_Y, y_pred=predictions_XGB)\n",
    "\n",
    "    #################### KNN ####################\n",
    "\n",
    "    # Randomized search for knn\n",
    "    \n",
    "    if iter == 0:\n",
    "        KNN = KNeighborsClassifier()\n",
    "\n",
    "        #Randomized Search\n",
    "        random_search = RandomizedSearchCV(KNN, knn_param_dist, n_iter=20, cv=5, scoring='roc_auc', random_state=42)\n",
    "        random_search.fit(train_X, train_Y)\n",
    "\n",
    "        print(\"KNN Best Parameters:\", random_search.best_params_)\n",
    "        print(\"KNN Best Score:\", random_search.best_score_)\n",
    "\n",
    "        knn_best_params = random_search.best_params_\n",
    "\n",
    "    else:\n",
    "        KNN = KNeighborsClassifier(\n",
    "            n_neighbors=knn_best_params['n_neighbors'],\n",
    "            weights=knn_best_params['weights'],\n",
    "            metric=knn_best_params['metric'])\n",
    "\n",
    "\n",
    "    KNN.fit(train_X,train_Y)\n",
    "\n",
    "    predictions_KNN = KNN.predict(test_X)\n",
    "\n",
    "    calculate_metrics(metrics_list=knn_metrics, y_true=test_Y, y_pred=predictions_KNN)\n",
    "\n",
    "    #################### MLP ####################\n",
    "\n",
    "    #sample_weight = np.array([class_weights[cls] for cls in train_Y])\n",
    "    #sample_weight = None\n",
    "    MLP =  MLPClassifier(hidden_layer_sizes=(num_bits), activation='relu', solver='adam', max_iter=200)\n",
    "    MLP.fit(train_X, train_Y)\n",
    "    predictions_MLP = MLP.predict(test_X)\n",
    "\n",
    "    calculate_metrics(metrics_list=mlp_metrics, y_true=test_Y, y_pred=predictions_MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #OLD\\nmetrics = [svm_metrics, rf_metrics, xgb_metrics, knn_metrics, mlp_metrics]\\nmetrics_np = np.zeros((len(metrics), 12))\\n\\nfor i, clf in enumerate(metrics):\\n    metrics_np[i, 0::2] = np.round([np.mean(metric) for metric in clf], 3)\\n    metrics_np[i, 1::2] = np.round([np.std(metric) for metric in clf], 3)    \\n\\n\\nmetric_names = [\\'AUC\\', \\'Accuracy\\', \\'F1 Score\\', \\'Precision\\', \\'Sensitivity\\', \\'Specificity\\']\\n\\ncolumns = []\\nclfs = [\"SVM\", \"RF\",\"XGB\", \"KNN\", \"MLP\"]\\nfor name in metric_names:\\n    columns.extend([f\\'Mean {name}\\', f\\'Std {name}\\'])\\n\\ndf_clfs = pd.DataFrame(clfs, columns=[\"Classifier\"])\\ndf_metrics = pd.DataFrame(metrics_np, columns=columns)\\ndf = pd.concat([df_clfs, df_metrics], axis=1)\\n\\nfilename = f\"ml_{dt_file.strip(\\'.csv\\')}_{fp_type}_{target_name}.csv\"\\ndf.to_csv(filename, index=False)\\nprint(filename)\\n '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #OLD\n",
    "metrics = [svm_metrics, rf_metrics, xgb_metrics, knn_metrics, mlp_metrics]\n",
    "metrics_np = np.zeros((len(metrics), 12))\n",
    "\n",
    "for i, clf in enumerate(metrics):\n",
    "    metrics_np[i, 0::2] = np.round([np.mean(metric) for metric in clf], 3)\n",
    "    metrics_np[i, 1::2] = np.round([np.std(metric) for metric in clf], 3)    \n",
    "\n",
    "\n",
    "metric_names = ['AUC', 'Accuracy', 'F1 Score', 'Precision', 'Sensitivity', 'Specificity']\n",
    "\n",
    "columns = []\n",
    "clfs = [\"SVM\", \"RF\",\"XGB\", \"KNN\", \"MLP\"]\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "df_clfs = pd.DataFrame(clfs, columns=[\"Classifier\"])\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "df = pd.concat([df_clfs, df_metrics], axis=1)\n",
    "\n",
    "filename = f\"ml_{dt_file.strip('.csv')}_{fp_type}_{target_name}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "print(filename)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.619 0.    0.609 0.    0.832 0.    0.387 0.    0.694 0.    0.596 0.   ]\n",
      " [0.614 0.007 0.604 0.007 0.846 0.008 0.362 0.012 0.695 0.005 0.59  0.005]\n",
      " [0.588 0.    0.574 0.    0.911 0.    0.237 0.    0.697 0.    0.564 0.   ]\n",
      " [0.567 0.    0.553 0.    0.901 0.    0.204 0.    0.684 0.    0.552 0.   ]\n",
      " [0.598 0.011 0.586 0.011 0.89  0.015 0.281 0.025 0.698 0.007 0.574 0.007]]\n",
      "results\\BBBP\\ml_maccs_p_np.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics = [svm_metrics, rf_metrics, xgb_metrics, knn_metrics, mlp_metrics]\n",
    "metrics_np = np.zeros((len(metrics), 12))\n",
    "\n",
    "for i, clf in enumerate(metrics):\n",
    "    metrics_np[i, 0::2] = np.round([np.mean(metric) for metric in clf], 3)\n",
    "    metrics_np[i, 1::2] = np.round([np.std(metric) for metric in clf], 3)  \n",
    "\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "print(metrics_np)\n",
    "clfs = [\"SVM\", \"RF\",\"XGB\", \"KNN\", \"MLP\"]\n",
    "df_clfs = pd.DataFrame(clfs, columns=[\"Classifier\"])\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "df = pd.concat([df_clfs, df_metrics], axis=1)\n",
    "\n",
    "if fp_type in ['maccs', 'pubchem']:\n",
    "    filename = f\"results\\\\{dirname}\\\\ml_{fp_type}_{target_name}.csv\"\n",
    "\n",
    "else:\n",
    "    filename = f\"results\\\\{dirname}\\\\ml_{fp_type}_{num_bits}_{target_name}.csv\"\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
