{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp, data_splitter, smile_to_fp_mix\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Hepatobiliary disorders', 'Metabolism and nutrition disorders',\n",
      "       'Product issues', 'Eye disorders', 'Investigations',\n",
      "       'Musculoskeletal and connective tissue disorders',\n",
      "       'Gastrointestinal disorders', 'Social circumstances',\n",
      "       'Immune system disorders', 'Reproductive system and breast disorders',\n",
      "       'Neoplasms benign, malignant and unspecified (incl cysts and polyps)',\n",
      "       'General disorders and administration site conditions',\n",
      "       'Endocrine disorders', 'Surgical and medical procedures',\n",
      "       'Vascular disorders', 'Blood and lymphatic system disorders',\n",
      "       'Skin and subcutaneous tissue disorders',\n",
      "       'Congenital, familial and genetic disorders',\n",
      "       'Infections and infestations',\n",
      "       'Respiratory, thoracic and mediastinal disorders',\n",
      "       'Psychiatric disorders', 'Renal and urinary disorders',\n",
      "       'Pregnancy, puerperium and perinatal conditions',\n",
      "       'Ear and labyrinth disorders', 'Cardiac disorders',\n",
      "       'Nervous system disorders',\n",
      "       'Injury, poisoning and procedural complications'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[1]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n",
    "print(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hepatobiliary disorders\n",
      "743\n",
      "1427\n"
     ]
    }
   ],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "elif dirname == 'sider':\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()\n",
    "\n",
    "print(target_name)\n",
    "print(df[target_name].sum())\n",
    "print(df[target_name].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILE to Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDKit - 512\n",
      "morgan - 512\n"
     ]
    }
   ],
   "source": [
    "fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['pubchem', 881]]\n",
    "mix = True\n",
    "fp_type, num_bits = fp_types[2]\n",
    "if mix and fp_type == 'RDKit':\n",
    "    num_bits = 512\n",
    "fp_config = {\"fp_type\": fp_type,\n",
    "             \"num_bits\": num_bits,\n",
    "             \"radius\": 2,\n",
    "             \"fp_type_2\": fp_types[0][0],\n",
    "             \"num_bits_2\": 1024 - num_bits,\n",
    "             \"mix\": mix,\n",
    "             }\n",
    "\n",
    "print(fp_type, '-', num_bits)\n",
    "if mix:\n",
    "   print(fp_config['fp_type_2'], '-', fp_config['num_bits_2']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:54:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:54:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "split = \"scaffold\"\n",
    "dataset = None\n",
    "\n",
    "if dirname != 'BBBP':\n",
    "    split = \"random\"\n",
    "    if fp_config['mix']:\n",
    "        fp_array, target_array = smile_to_fp_mix(df, fp_config=fp_config, target_name=target_name)\n",
    "    else:\n",
    "        fp_array, target_array = smile_to_fp(df, fp_config=fp_config, target_name=target_name)\n",
    "    # Create Torch Dataset\n",
    "    fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "    target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "    dataset = TensorDataset(fp_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics ---- roc  acc f1  prs sns sps\n",
    "svm_metrics = [[], [], [], [], [], []]\n",
    "rf_metrics  = [[], [], [], [], [], []]\n",
    "knn_metrics = [[], [], [], [], [], []]\n",
    "xgb_metrics = [[], [], [], [], [], []]\n",
    "mlp_metrics = [[], [], [], [], [], []]\n",
    "metrics = [svm_metrics, rf_metrics, xgb_metrics, knn_metrics, mlp_metrics]\n",
    "\n",
    "grid_parameters = {\n",
    "    \"SVM\": {\n",
    "        \"C\": list(range(1, 100)),\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"],\n",
    "        \"degree\": [2, 3, 4],\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"max_depth\": [5] + list(range(10, 100, 10)),\n",
    "        \"n_estimators\": list(range(50, 400, 50)),\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"learning_rate\": [0.005, 0.01, 0.1, 0.2],\n",
    "        \"max_depth\": range(2, 20, 2),\n",
    "        \"n_estimators\": range(50, 400, 50),\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": list(range(1, 20)),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "knn_best_params = []\n",
    "svm_best_params = []\n",
    "rf_best_params = []\n",
    "xgb_best_params = []\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(metrics_list, y_pred, y_true):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, train_X, train_Y, test_X, test_Y, metrics_list):\n",
    "    model.fit(train_X,train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)\n",
    "    \n",
    "    calculate_metrics(metrics_list=metrics_list, y_true=test_Y, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_param_search(model, grid_param, train_X, train_Y):\n",
    "    search = RandomizedSearchCV(model, grid_param, n_iter=40, scoring='roc_auc', random_state=42)\n",
    "    search.fit(train_X, train_Y)\n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_weight = (sum(train_Y == 1) / sum(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      "0/30\n",
      "XGBoost parameter search...\n",
      "KNN parameter search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"C:\\Users\\knsve\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\subprocess.py\", line 501, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"C:\\Users\\knsve\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\subprocess.py\", line 966, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\knsve\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\subprocess.py\", line 1435, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM parameter search...\n",
      "Random Forest parameter search...\n",
      "{'weights': 'distance', 'n_neighbors': 8, 'metric': 'manhattan'} {'kernel': 'poly', 'gamma': 'auto', 'degree': 2, 'C': 22} {'n_estimators': 350, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 10} {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.01}\n",
      "1/30\n",
      "2/30\n",
      "3/30\n",
      "4/30\n",
      "5/30\n",
      "6/30\n",
      "7/30\n",
      "8/30\n",
      "9/30\n",
      "10/30\n",
      "11/30\n",
      "12/30\n",
      "13/30\n",
      "14/30\n",
      "15/30\n",
      "16/30\n",
      "17/30\n",
      "18/30\n",
      "19/30\n",
      "20/30\n",
      "21/30\n",
      "22/30\n",
      "23/30\n",
      "24/30\n",
      "25/30\n",
      "26/30\n",
      "27/30\n",
      "28/30\n",
      "29/30\n"
     ]
    }
   ],
   "source": [
    "iterations = 30\n",
    "print(\"Iterations:\")\n",
    "for iter in range(iterations):\n",
    "    print(str(iter) + \"/30\")\n",
    "    seed = iter+1\n",
    "    random.seed(seed)\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, fp_config=fp_config, seed=iter+1, dtype=dtype)\n",
    "    train_X, train_Y = train[:]\n",
    "    val_X, val_Y = val[:]\n",
    "    test_X, test_Y = test[:]\n",
    "    \n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.array(train_Y))\n",
    "    #class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    class_weights_dict = \"balanced\"\n",
    "\n",
    "    pos_weight = (sum(train_Y == 1).numpy() / sum(train_Y == 0).numpy())\n",
    "\n",
    "\n",
    "    if iter == 0:\n",
    "        print(\"XGBoost parameter search...\")\n",
    "        XGB = XGBClassifier(objective=\"binary:logistic\", scale_pos_weight=pos_weight)\n",
    "        xgb_best_params = random_param_search(XGB, grid_parameters['XGB'], train_X, train_Y)\n",
    "        print(\"KNN parameter search...\")\n",
    "        KNN = KNeighborsClassifier()\n",
    "        knn_best_params = random_param_search(KNN, grid_parameters['KNN'], train_X, train_Y)\n",
    "        print(\"SVM parameter search...\")\n",
    "        SVM = svm.SVC(class_weight=class_weights_dict, random_state=seed)\n",
    "        svm_best_params = random_param_search(SVM, grid_parameters['SVM'], train_X, train_Y)\n",
    "        print(\"Random Forest parameter search...\")\n",
    "        RF = RandomForestClassifier(class_weight=class_weights_dict, random_state=seed)\n",
    "        rf_best_params = random_param_search(RF, grid_parameters['RF'], train_X, train_Y)\n",
    "\n",
    "        print(knn_best_params, svm_best_params, rf_best_params, xgb_best_params)\n",
    "\n",
    "    SVM = svm.SVC(**svm_best_params, class_weight=class_weights_dict, random_state=seed)\n",
    "    RF = RandomForestClassifier(**rf_best_params, class_weight=class_weights_dict, random_state=seed)\n",
    "    XGB = XGBClassifier(**xgb_best_params, objective=\"binary:logistic\", scale_pos_weight=pos_weight, random_state=seed)\n",
    "    KNN = KNeighborsClassifier(**knn_best_params)\n",
    "    MLP =  MLPClassifier(hidden_layer_sizes=(num_bits), activation='relu', solver='adam', max_iter=1000)\n",
    "\n",
    "    models = [SVM, RF, XGB, KNN, MLP]\n",
    "    for i, model in enumerate(models):\n",
    "        train_test_model(model, train_X, train_Y, test_X, test_Y, metrics[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.669 0.037 0.669 0.037 0.665 0.047 0.674 0.048 0.68  0.038 0.697 0.044]\n",
      " [0.699 0.037 0.692 0.036 0.808 0.047 0.576 0.047 0.739 0.036 0.682 0.042]\n",
      " [0.674 0.028 0.669 0.028 0.766 0.045 0.571 0.056 0.713 0.029 0.669 0.038]\n",
      " [0.698 0.039 0.693 0.038 0.774 0.054 0.612 0.044 0.73  0.04  0.692 0.041]\n",
      " [0.659 0.038 0.658 0.037 0.673 0.052 0.642 0.051 0.676 0.04  0.68  0.045]]\n",
      "results\\sider\\ml_RDKit_morgan_Hepatobiliary disorders.csv\n"
     ]
    }
   ],
   "source": [
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics = [svm_metrics, rf_metrics, xgb_metrics, knn_metrics, mlp_metrics]\n",
    "metrics_np = np.zeros((len(metrics), 12))\n",
    "\n",
    "for i, clf in enumerate(metrics):\n",
    "    metrics_np[i, 0::2] = np.round([np.mean(metric) for metric in clf], 3)\n",
    "    metrics_np[i, 1::2] = np.round([np.std(metric) for metric in clf], 3)  \n",
    "\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "print(metrics_np)\n",
    "clfs = [\"SVM\", \"RF\",\"XGB\", \"KNN\", \"MLP\"]\n",
    "df_clfs = pd.DataFrame(clfs, columns=[\"Classifier\"])\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "df = pd.concat([df_clfs, df_metrics], axis=1)\n",
    "\n",
    "if fp_config['mix']:\n",
    "    filename = f\"results\\\\{dirname}\\\\ml_{fp_type}_{fp_config['fp_type_2']}_{target_name}.csv\"\n",
    "\n",
    "elif fp_type in ['maccs', 'pubchem']:\n",
    "    filename = f\"results\\\\{dirname}\\\\ml_{fp_type}_{target_name}.csv\"\n",
    "\n",
    "else:\n",
    "    filename = f\"results\\\\{dirname}\\\\ml_{fp_type}_{num_bits}_{target_name}.csv\"\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
