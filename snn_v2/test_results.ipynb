{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n",
      "c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import load_dataset_df,  data_splitter, get_spiking_net, make_filename, smiles_to_feat\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from csnn_model import get_prediction_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[0]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "    # SR-MMP\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders 1427 samples, 0.52 class ratio\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Molecular Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"fp\", \"descriptor\", \"SELFIES-1hot\", \"SMILES-1hot\"]\n",
    "\n",
    "repr_type = representations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptor\n"
     ]
    }
   ],
   "source": [
    "if repr_type == \"fp\":\n",
    "    fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['count_morgan', 1024], ['pubchem', 881]]\n",
    "    mix = False\n",
    "    fp_type, num_bits = fp_types[1]\n",
    "    if mix and fp_type == 'RDKit':\n",
    "        num_bits = 512\n",
    "    data_config = {\"fp_type\": fp_type,\n",
    "                \"num_bits\": num_bits,\n",
    "                \"radius\": 2,\n",
    "                \"fp_type_2\": fp_types[0][0],\n",
    "                \"num_bits_2\": 1024 - num_bits,\n",
    "                \"mix\": mix,\n",
    "                \"dim_2\": False}\n",
    "    dim_2 = data_config['dim_2']\n",
    "    print(fp_type, '-', num_bits)\n",
    "    if mix: print(data_config['fp_type_2'], '-', data_config['num_bits_2'])\n",
    "    if dim_2: print(\"2D FP\")\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    desc_type = [\"RDKit\", \"Mordred\"]\n",
    "    data_config = {\"desc_type\": desc_type[0],\n",
    "                   \"size\": 0,\n",
    "                }\n",
    "\n",
    "data_config[\"repr_type\"] = repr_type\n",
    "print(repr_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:56:32] Explicit valence for atom # 8 Al, 6, is greater than permitted\n",
      "[14:56:32] Explicit valence for atom # 3 Al, 6, is greater than permitted\n",
      "[14:56:32] Explicit valence for atom # 4 Al, 6, is greater than permitted\n",
      "[14:56:32] Explicit valence for atom # 4 Al, 6, is greater than permitted\n",
      "[14:56:32] Explicit valence for atom # 9 Al, 6, is greater than permitted\n",
      "[14:56:32] Explicit valence for atom # 5 Al, 6, is greater than permitted\n",
      "[14:56:32] Explicit valence for atom # 16 Al, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_array shape: (5825, 217)\n",
      "valid_df shape: (5825, 2)\n",
      "Inf in descriptor MaxPartialCharge for molecule CC1=C2N=C(C=C3N=C(C(C)=C4[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@](C)([C@@H]5N=C1[C@](C)(CCC(=O)NC[C@@H](C)OP(=O)([O-])O[C@@H]1[C@@H](CO)O[C@H](n6cnc7cc(C)c(C)cc76)[C@@H]1O)[C@H]5CC(N)=O)N4[Co+]C#N)[C@@](C)(CC(N)=O)[C@@H]3CCC(N)=O)C(C)(C)[C@@H]2CCC(N)=O\n",
      "Inf in descriptor MinPartialCharge for molecule CC1=C2N=C(C=C3N=C(C(C)=C4[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@](C)([C@@H]5N=C1[C@](C)(CCC(=O)NC[C@@H](C)OP(=O)([O-])O[C@@H]1[C@@H](CO)O[C@H](n6cnc7cc(C)c(C)cc76)[C@@H]1O)[C@H]5CC(N)=O)N4[Co+]C#N)[C@@](C)(CC(N)=O)[C@@H]3CCC(N)=O)C(C)(C)[C@@H]2CCC(N)=O\n",
      "Inf in descriptor MaxAbsPartialCharge for molecule CC1=C2N=C(C=C3N=C(C(C)=C4[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@](C)([C@@H]5N=C1[C@](C)(CCC(=O)NC[C@@H](C)OP(=O)([O-])O[C@@H]1[C@@H](CO)O[C@H](n6cnc7cc(C)c(C)cc76)[C@@H]1O)[C@H]5CC(N)=O)N4[Co+]C#N)[C@@](C)(CC(N)=O)[C@@H]3CCC(N)=O)C(C)(C)[C@@H]2CCC(N)=O\n",
      "Inf in descriptor MinAbsPartialCharge for molecule CC1=C2N=C(C=C3N=C(C(C)=C4[C@@H](CCC(N)=O)[C@](C)(CC(N)=O)[C@](C)([C@@H]5N=C1[C@](C)(CCC(=O)NC[C@@H](C)OP(=O)([O-])O[C@@H]1[C@@H](CO)O[C@H](n6cnc7cc(C)c(C)cc76)[C@@H]1O)[C@H]5CC(N)=O)N4[Co+]C#N)[C@@](C)(CC(N)=O)[C@@H]3CCC(N)=O)C(C)(C)[C@@H]2CCC(N)=O\n",
      "Inf in descriptor MaxPartialCharge for molecule CCCCCCCCCCCC(=O)O[Sn](CCCC)(CCCC)OC(=O)CCCCCCCCCCC\n",
      "Inf in descriptor MaxAbsPartialCharge for molecule CCCCCCCCCCCC(=O)O[Sn](CCCC)(CCCC)OC(=O)CCCCCCCCCCC\n",
      "torch.Size([5825, 217])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "split = \"scaffold\" if dirname == \"BBBP\" else \"random\"\n",
    "dataset = None\n",
    "\n",
    "\n",
    "feat_tensor, target_tensor, feat_df = smiles_to_feat(df,repr_type=repr_type, data_config=data_config, target_name=target_name, dtype=dtype)\n",
    "print(feat_tensor.shape)\n",
    "dataset = TensorDataset(feat_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_loss\n"
     ]
    }
   ],
   "source": [
    "loss_types = ['ce_mem', 'rate_loss', 'count_loss', 'temporal_loss', 'bce_loss']\n",
    "loss_type = loss_types[2]\n",
    "print(loss_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNN\n"
     ]
    }
   ],
   "source": [
    "net_types = [\"SNN\", \"DSNN\", \"CSNN\", \"RSNN\"]\n",
    "net_type = net_types[0]\n",
    "slope = 10\n",
    "#spike_grad = surrogate.fast_sigmoid(slope=slope)\n",
    "spike_grad = None\n",
    "beta = 0.95\n",
    "bias = True\n",
    "net_config = {\n",
    "            \"num_hidden\": 1024,\n",
    "            \"num_hidden_l2\": 256,\n",
    "            \"num_steps\": 10,\n",
    "            \"spike_grad\": spike_grad,\n",
    "            \"slope\": None if not spike_grad else slope, #spike_grad.__closure__[0].cell_contents,\n",
    "            \"beta\": beta,\n",
    "            \"encoding\": 'rate' if loss_type != 'temporal_loss' else 'ttfs',\n",
    "            \"bias\": bias,\n",
    "            \"out_num\": 2,\n",
    "            \"num_hidden_layers\": 2,\n",
    "            \"num_hidden_l3\": 256,\n",
    "            }\n",
    "if net_type == \"CSNN\":\n",
    "    net_config['num_conv'] = 1\n",
    "    net_config[\"pool_size\"] = 2\n",
    "    net_config[\"conv_kernel\"] = 3\n",
    "    net_config[\"conv_stride\"] = [1 for _ in range(net_config['num_conv'])]\n",
    "    net_config[\"conv_groups\"] = 1\n",
    "\n",
    "if repr_type == \"fp\":\n",
    "    net_config[\"input_size\"] = 1024 if data_config['mix'] else num_bits\n",
    "    net_config[\"2d\"] = data_config['dim_2']\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    net_config[\"input_size\"] = feat_tensor.shape[1]\n",
    "    net_config[\"2d\"] = False\n",
    "    #net_config[\"num_steps\"] = 10\n",
    "\n",
    "print(net_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "<function prediction_spk_rate_scores at 0x000002CF16FAE8C0>\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "pop_coding = net_config['out_num'] > 2\n",
    "lr=1e-4 #1e-6 default for 1000 epochs. csnn requires higher\n",
    "iterations = 30\n",
    "weight_decay = 0 # 1e-5\n",
    "#weight_decay = 1e-4\n",
    "optim_type = 'Adam'\n",
    "#optim_type = 'SGD'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "batch_size = 16 #16, 8\n",
    "scores = True\n",
    "train_config = {\"num_epochs\": 100,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['num_steps'],\n",
    "                'val_net': None,\n",
    "                'prediction_fn': get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding, scores=scores),\n",
    "                }\n",
    "drop_last = net_type == \"CSNN\"\n",
    "pin_memory = device == \"cuda\"\n",
    "save_csv = True\n",
    "save_models = True\n",
    "print(train_config[\"prediction_fn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(metrics_list, all_targets, all_preds):\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    all_preds = np.array(all_preds) > 0.0\n",
    "    \n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    \n",
    "    print(\"acc:\", accuracy,\"auc:\", auc_roc)\n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217 1024\n",
      "1024 2\n",
      "results\\tox21\\models\\\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-1.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-2.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-3.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-4.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-5.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-6.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-7.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-8.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-9.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-10.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-11.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-12.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-13.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-14.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-15.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-16.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-17.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-18.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-19.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-20.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-21.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-22.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-23.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-24.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-25.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-26.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-27.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-28.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-29.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-30.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "net_list = []\n",
    "    \n",
    "net, train_net, val_net, test_net = get_spiking_net(net_type, net_config)\n",
    "filename = make_filename(dirname, target_name, net_type, data_config, lr, weight_decay, optim_type, net_config, train_config, net, model = True)\n",
    "\n",
    "model_name = filename.removesuffix('.csv')\n",
    "\n",
    "models_path = os.path.join(\"results\", dirname, \"models\", \"\")\n",
    "all_model_names = os.listdir(models_path)\n",
    "print(models_path)\n",
    "#print(all_model_names)\n",
    "for iter in range(iterations):\n",
    "    seed = int(iter + 1)\n",
    "    string_id = f\"seed-{seed}.pth\"\n",
    "    search_name = model_name + str(string_id) \n",
    "    search_name_no_folder = search_name.removeprefix(models_path)\n",
    "    if search_name_no_folder in all_model_names:\n",
    "        state_dict = torch.load(search_name, weights_only=True)\n",
    "        net_list.append(copy.deepcopy(state_dict))\n",
    "    else: print(search_name_no_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-1_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-2_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-3_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-4_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-5_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-6_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-7_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-8_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-9_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-10_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-11_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-12_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-13_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-14_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-15_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-16_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-17_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-18_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-19_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-20_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-21_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-22_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-23_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-24_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-25_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-26_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-27_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-28_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-29_auc.pth\n",
      "SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_biasseed-30_auc.pth\n"
     ]
    }
   ],
   "source": [
    "if len(net_list) < 30:\n",
    "    net_list = []\n",
    "    for iter in range(iterations):\n",
    "        seed = int(iter + 1)\n",
    "        string_id = f\"seed-{seed}_auc.pth\"\n",
    "        search_name = model_name + str(string_id) \n",
    "        search_name_no_folder = search_name.removeprefix(models_path)\n",
    "        if search_name_no_folder in all_model_names:\n",
    "            state_dict = torch.load(search_name, weights_only=True)\n",
    "            net_list.append(copy.deepcopy(state_dict))\n",
    "        else: print(search_name_no_folder)\n",
    "\n",
    "if len(net_list) < 30:\n",
    "    net_list = []\n",
    "    for iter in range(iterations):\n",
    "        seed = int(iter + 1)\n",
    "        string_id = f\"-scores-seed-{seed}.pth\"\n",
    "        search_name = model_name + str(string_id) \n",
    "        search_name_no_folder = search_name.removeprefix(models_path)\n",
    "        if search_name_no_folder in all_model_names:\n",
    "            state_dict = torch.load(search_name, weights_only=True)\n",
    "            net_list.append(copy.deepcopy(state_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_norm(train_subset, val_subset, test_subset):\n",
    "    train_tensor, _ = train_subset[:]\n",
    "    val_tensor, _ = val_subset[:]\n",
    "    test_tensor, _ = test_subset[:]\n",
    "\n",
    "    min_val = train_tensor.min(dim=0).values\n",
    "    max_val = train_tensor.max(dim=0).values\n",
    "    range_val = (max_val - min_val).clamp(min=1e-6)\n",
    "\n",
    "    train_norm = ((train_tensor - min_val) / range_val).clamp(0.0, 1.0)\n",
    "    val_norm   = ((val_tensor   - min_val) / range_val).clamp(0.0, 1.0)\n",
    "    test_norm  = ((test_tensor  - min_val) / range_val).clamp(0.0, 1.0)\n",
    "\n",
    "    return train_norm, val_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:1 -> acc: 0.7353951890034365 auc: 0.6521891274366522\n",
      "Seed:2 -> acc: 0.7491408934707904 auc: 0.653262257534913\n",
      "Seed:3 -> acc: 0.781786941580756 auc: 0.6674280979997633\n",
      "Seed:4 -> acc: 0.7731958762886598 auc: 0.6844878048780489\n",
      "Seed:5 -> acc: 0.7491408934707904 auc: 0.6631720491373613\n",
      "Seed:6 -> acc: 0.7766323024054983 auc: 0.7168580212058473\n",
      "Seed:7 -> acc: 0.7749140893470791 auc: 0.6660962115528882\n",
      "Seed:8 -> acc: 0.7749140893470791 auc: 0.6523173479240426\n",
      "Seed:9 -> acc: 0.6993127147766323 auc: 0.6716475095785441\n",
      "Seed:10 -> acc: 0.788659793814433 auc: 0.6514792899408285\n",
      "Seed:11 -> acc: 0.781786941580756 auc: 0.6613222304011779\n",
      "Seed:12 -> acc: 0.8384879725085911 auc: 0.7033945986496624\n",
      "Seed:13 -> acc: 0.761168384879725 auc: 0.6750115442971172\n",
      "Seed:14 -> acc: 0.7302405498281787 auc: 0.6452574525745257\n",
      "Seed:15 -> acc: 0.7852233676975945 auc: 0.6791409465020577\n",
      "Seed:16 -> acc: 0.7474226804123711 auc: 0.6877760397497239\n",
      "Seed:17 -> acc: 0.7405498281786942 auc: 0.6313757525570153\n",
      "Seed:18 -> acc: 0.7508591065292096 auc: 0.7029668049792531\n",
      "Seed:19 -> acc: 0.761168384879725 auc: 0.6878309737382471\n",
      "Seed:20 -> acc: 0.7164948453608248 auc: 0.663454830833624\n",
      "Seed:21 -> acc: 0.7852233676975945 auc: 0.6356670105946599\n",
      "Seed:22 -> acc: 0.7096219931271478 auc: 0.6829945799457994\n",
      "Seed:23 -> acc: 0.7542955326460481 auc: 0.671014806008862\n",
      "Seed:24 -> acc: 0.7938144329896907 auc: 0.6348578957274609\n",
      "Seed:25 -> acc: 0.7663230240549829 auc: 0.6778790389395195\n",
      "Seed:26 -> acc: 0.7869415807560137 auc: 0.6082474226804124\n",
      "Seed:27 -> acc: 0.7731958762886598 auc: 0.6596763725073297\n",
      "Seed:28 -> acc: 0.7525773195876289 auc: 0.6519325887961326\n",
      "Seed:29 -> acc: 0.7955326460481099 auc: 0.6773908372686377\n",
      "Seed:30 -> acc: 0.8127147766323024 auc: 0.6561924917188076\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "results = [[], [], [], [], [], []]\n",
    "for iter in range(iterations):\n",
    "    #print(f\"Iteration:{iter + 1}/{iterations}\")\n",
    "    seed = iter + 1\n",
    "    print(f\"Seed:{seed} -> \",end='', flush=True)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # DATA SPLIT\n",
    "    train, val, test = data_splitter(feat_df, target_name, split=split, dataset=dataset, data_config=data_config, seed=seed, dtype=dtype)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "\n",
    "    if repr_type == \"descriptor\":\n",
    "        train_data, val_data, test_data = minmax_norm(train, val, test)\n",
    "        train = TensorDataset(train_data, train_label)\n",
    "        val = TensorDataset(val_data,val_label)\n",
    "        test = TensorDataset(test_data, test_label)\n",
    "\n",
    "    #train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    #val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # TESTING\n",
    "    model = net\n",
    "    flag = False\n",
    "    try:\n",
    "        loaded_result = model.load_state_dict(net_list[iter])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"{e}\")\n",
    "        model.load_state_dict(net_list[iter], strict=False)\n",
    "        flag = True\n",
    "    model.to(device)\n",
    "    all_preds, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    calc_metrics(results, all_preds=all_preds, all_targets=all_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.765 ± 0.029\n",
      "AUC ROC: 0.666 ± 0.023\n",
      "Sensitivity: 0.521 ± 0.070\n",
      "Specificity: 0.810 ± 0.043\n"
     ]
    }
   ],
   "source": [
    "metrics_np = np.zeros(12)\n",
    "\n",
    "for i, metric in enumerate(results):\n",
    "    metrics_np[i*2] = np.round(np.mean(metric), 3)\n",
    "    metrics_np[i*2+1] = np.round(np.std(metric), 3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy:  {metrics_np[0]:.3f} ± {metrics_np[1]:.3f}\")\n",
    "print(f\"AUC ROC: {metrics_np[2]:.3f} ± {metrics_np[3]:.3f}\")\n",
    "print(f\"Sensitivity: {metrics_np[4]:.3f} ± {metrics_np[5]:.3f}\")\n",
    "print(f\"Specificity: {metrics_np[6]:.3f} ± {metrics_np[7]:.3f}\")\n",
    "\n",
    "\n",
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics_np = metrics_np.reshape(1, -1)\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "num_hidden = net_config['num_hidden']\n",
    "time_steps = train_config['num_steps']\n",
    "num_epochs = train_config['num_epochs']\n",
    "\n",
    "df_raw = pd.DataFrame({name: results[i] for i, name in enumerate(metric_names)})\n",
    "df_raw[\"Seed\"] = list(range(1, 31))\n",
    "df_raw = df_raw[[\"Seed\"] + metric_names]  # reorder columns\n",
    "\n",
    "blank = pd.DataFrame([[\"\"] * 12] * 3, columns=columns) \n",
    "save = True\n",
    "filename = make_filename(dirname, target_name, net_type, data_config, lr, weight_decay, optim_type, net_config, train_config, model)\n",
    "save = not flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_2\\tox21\\SR-ARE_SNN_beta-0.95_desc_217_l11024_t10_e100_b16_lr0.0001_count_loss_Adam_wd0_bias_no_scores.scv\n"
     ]
    }
   ],
   "source": [
    "filename = filename.replace(\"results\", \"results_2\")\n",
    "filename = filename.replace(\".csv\", \"_no_scores.scv\")\n",
    "if save: \n",
    "    df_metrics.to_csv(filename, index=False)\n",
    "    blank.to_csv(filename, mode='a', index=False, header=False)\n",
    "    df_raw.to_csv(filename, mode='a', index=False)\n",
    "\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
