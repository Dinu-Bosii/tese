{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f505574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. SNN-Grad + SmoothGrad\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def simple_snn_grad(model, x, target_class=0):\n",
    "    model.eval()\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "    output = model(x)  # shape [1, 2]\n",
    "    score = output[0, target_class]\n",
    "\n",
    "    model.zero_grad()\n",
    "    score.backward()\n",
    "\n",
    "    grad = x.grad.detach().clone()\n",
    "    return grad[0]  # shape [1024]\n",
    "\n",
    "#n samples is the num of noisy copies used to create the avg gradients. To account for noisyness.\n",
    "def snn_grad_smoothgrad(model, x, target_class=0, n_samples=50, noise_std=0.01):\n",
    "    model.eval()\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "    grads = []\n",
    "    for _ in range(n_samples):\n",
    "        noise = torch.randn_like(x) * noise_std\n",
    "        x_noisy = x + noise\n",
    "        x_noisy.requires_grad_(True)\n",
    "\n",
    "        output = model(x_noisy)  # shape [1, 2]\n",
    "        score = output[0, target_class]\n",
    "        model.zero_grad()\n",
    "        score.backward(retain_graph=True)\n",
    "        grads.append(x_noisy.grad.detach().clone())\n",
    "\n",
    "    avg_grad = torch.stack(grads).mean(dim=0)  # average gradients\n",
    "    return avg_grad[0]  # shape [1024]\n",
    "\n",
    "#2. Feature Ablation\n",
    "\n",
    "def feature_ablation(model, x, target_class=0):\n",
    "    model.eval()\n",
    "    x = x.clone().detach()\n",
    "    baseline_pred = model(x)[0, target_class].item()\n",
    "\n",
    "    importances = []\n",
    "    for i in range(x.shape[1]):\n",
    "        x_ablated = x.clone()\n",
    "        x_ablated[0, i] = 0.0  # zero out feature i\n",
    "        with torch.no_grad():\n",
    "            pred_ablated = model(x_ablated)[0, target_class].item()\n",
    "        importance = baseline_pred - pred_ablated\n",
    "        importances.append(importance)\n",
    "\n",
    "    return torch.tensor(importances)  # shape [1024]\n",
    "\n",
    "#3. Integrated Gradients (IG)\n",
    "\n",
    "def integrated_gradients(model, x, baseline=None, target_class=0, steps=50):\n",
    "    model.eval()\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(x)\n",
    "\n",
    "    x = x.clone().detach()\n",
    "    baseline = baseline.clone().detach()\n",
    "\n",
    "    scaled_inputs = [baseline + (float(i) / steps) * (x - baseline) for i in range(steps + 1)]\n",
    "    grads = []\n",
    "\n",
    "    for scaled_x in scaled_inputs:\n",
    "        scaled_x.requires_grad_(True)\n",
    "        output = model(scaled_x)\n",
    "        score = output[0, target_class]\n",
    "        model.zero_grad()\n",
    "        score.backward(retain_graph=True)\n",
    "        grads.append(scaled_x.grad.detach().clone())\n",
    "\n",
    "    avg_grads = torch.stack(grads).mean(dim=0)\n",
    "    integrated_grads = (x - baseline) * avg_grads\n",
    "    return integrated_grads[0]  # shape [1024]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
