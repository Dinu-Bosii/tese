{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n",
      "c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from snn_model import get_loss_fn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp,smiles_to_descriptor,smiles_to_onehot,smiles_to_onehot_selfies,data_splitter,get_spiking_net,make_filename\n",
    "from utils import smiles_to_feat\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from csnn_model import get_prediction_fn\n",
    "from snntorch import surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = f\"./results/logs/output_{timestamp}.txt\"\n",
    "log_file = open(log_path, \"w\")\n",
    "sys.stdout = log_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[1]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n",
    "\n",
    "for t in targets:\n",
    "    df_temp = df[[t, 'smiles']].dropna()\n",
    "    class_counts = df[t].count()\n",
    "    class_sum = df[t].sum()\n",
    "    print(t, class_counts, round(class_sum/class_counts, 2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "    # SR-MMP\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders 1427 samples, 0.52 class ratio\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Molecular Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"fp\", \"descriptor\", \"SELFIES-1hot\", \"SMILES-1hot\"]#, \"graph-list\"]\n",
    "\n",
    "repr_type = representations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repr_type == \"fp\":\n",
    "    fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['count_morgan', 1024], ['pubchem', 881]]\n",
    "    mix = False\n",
    "    fp_type, num_bits = fp_types[0]\n",
    "    if mix and fp_type == 'RDKit':\n",
    "        num_bits = 512\n",
    "    data_config = {\"fp_type\": fp_type,\n",
    "                \"num_bits\": num_bits,\n",
    "                \"radius\": 2,\n",
    "                \"fp_type_2\": fp_types[0][0],\n",
    "                \"num_bits_2\": 1024 - num_bits,\n",
    "                \"mix\": mix,\n",
    "                \"dim_2\": False}\n",
    "    dim_2 = False\n",
    "    print(fp_type, '-', num_bits)\n",
    "    if mix: print(data_config['fp_type_2'], '-', data_config['num_bits_2'])\n",
    "    if dim_2: print(\"2D FP\")\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    desc_type = [\"RDKit\", \"Mordred\"]\n",
    "    data_config = {\"desc_type\": desc_type[1],\n",
    "                   \"size\": 0,\n",
    "                }\n",
    "\n",
    "data_config[\"repr_type\"] = repr_type\n",
    "print(repr_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:31:29] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "split = \"scaffold\" if dirname == \"BBBP\" else \"random\"\n",
    "dataset = None\n",
    "\n",
    "\n",
    "feat_tensor, target_tensor, feat_df = smiles_to_feat(df,repr_type=repr_type, data_config=data_config, target_name=target_name, dtype=dtype)\n",
    "print(feat_tensor.shape)\n",
    "dataset = TensorDataset(feat_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "loss_types = ['ce_mem', 'rate_loss', 'count_loss', 'temporal_loss', 'bce_loss']\n",
    "loss_type = loss_types[2]\n",
    "print(loss_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_types = [\"SNN\", \"DSNN\", \"CSNN\", \"RSNN\"]\n",
    "net_type = net_types[0]\n",
    "slope = 10\n",
    "spike_grad = surrogate.fast_sigmoid(slope=slope)\n",
    "#spike_grad = None\n",
    "beta = 0.95 \n",
    "bias = True\n",
    "net_config = {\n",
    "            \"num_hidden\": 1024,\n",
    "            \"num_hidden_l2\": 256,\n",
    "            \"num_steps\": 10,\n",
    "            \"spike_grad\": spike_grad,\n",
    "            \"slope\": None if not spike_grad else slope, #spike_grad.__closure__[0].cell_contents,\n",
    "            \"beta\": beta,\n",
    "            \"encoding\": 'rate' if loss_type != 'temporal_loss' else 'ttfs',\n",
    "            \"bias\": bias,\n",
    "            \"out_num\": 2\n",
    "            }\n",
    "#print(spike_grad.__closure__[0].cell_contents)\n",
    "if net_type == \"CSNN\":\n",
    "    net_config['num_conv'] = 1\n",
    "    net_config['conv_stride'] = [1 for _ in range(net_config['num_conv'])]\n",
    "    net_config[\"pool_size\"] = 2\n",
    "    net_config[\"conv_kernel\"] = 3\n",
    "    #net_config[\"conv_stride\"] = 1\n",
    "    net_config[\"conv_groups\"] = 1\n",
    "\n",
    "if repr_type == \"fp\":\n",
    "    net_config[\"input_size\"] = 1024 if data_config['mix'] else num_bits\n",
    "    net_config[\"2d\"] = dim_2\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    net_config[\"input_size\"] = desc_tensor.shape[1]\n",
    "    net_config[\"2d\"] = False\n",
    "    net_config[\"time_steps\"] = 10\n",
    "\n",
    "if repr_type == \"SELFIES-1hot\":\n",
    "    net_config[\"input_size\"] = [desc_tensor.shape[1],desc_tensor.shape[2]] \n",
    "    net_config[\"2d\"] = True\n",
    "if repr_type == \"SMILES-1hot\":\n",
    "    net_config[\"2d\"] = True\n",
    "    net_config[\"input_size\"] = [desc_tensor.shape[1],desc_tensor.shape[2]] \n",
    "print(net_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pop_coding = net_config['out_num'] > 2\n",
    "lr=1e-4 #1e-6 default for 1000 epochs. csnn requires higher\n",
    "iterations = 30\n",
    "weight_decay = 0 # 1e-5\n",
    "optim_type = 'Adam'\n",
    "#optim_type = 'SGD'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "batch_size = 16 #16, 8\n",
    "train_config = {\"num_epochs\": 1000,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['num_steps'],\n",
    "                'val_net': None,\n",
    "                'prediction_fn': get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding),\n",
    "                }\n",
    "drop_last = net_type == \"CSNN\"\n",
    "pin_memory = device == \"cuda\"\n",
    "save_csv = True\n",
    "save_models = True\n",
    "results = [[], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----Configuration-----\")\n",
    "print(data_config)\n",
    "print(net_config)\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "\n",
    "# Disable RDKit logging for the scaffold meeting\n",
    "RDLogger.DisableLog('rdApp.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(metrics_list, all_targets, all_preds):\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    \n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "# for CSNN\n",
    "if net_type == \"CSNN\":\n",
    "    search_space = OrderedDict({\n",
    "        \"bias\": [True, False],\n",
    "        \"beta\": [0.9, 0.7, 0.5],\n",
    "        \"learning_rate\": [1e-3, 1e-4, 1e-5],\n",
    "        \"out_num\": [2, 10, 20, 50],\n",
    "        \"spike_grad\": [None, surrogate.fast_sigmoid(slope=10), surrogate.fast_sigmoid(slope=25), surrogate.fast_sigmoid(slope=50)],\n",
    "        \"conv_groups\": [1, 2],\n",
    "        \"conv_kernel\": [3, 5, 7],\n",
    "        \"pool_size\": [2, 4],\n",
    "        \"conv_stride\": [1, 2],\n",
    "        #\"loss_fn\": ['ce_mem', 'rate_loss', 'count_loss'],\n",
    "        \"num_steps\": [5, 10, 20, 50]\n",
    "    })\n",
    "    net_config = {\n",
    "        \"beta\": 0.9,\n",
    "        \"spike_grad\": None,\n",
    "        \"slope\": slope,\n",
    "        \"encoding\": 'rate',\n",
    "        \"out_num\": 2,\n",
    "        'num_conv': 1,\n",
    "        #\"out_num\": params['out_num'],\n",
    "        #\"learning_rate\": 1e-4,\n",
    "        #\"pool_size\" : 2,\n",
    "        #\"conv_kernel\": params[\"conv_kernel\"],\n",
    "        #\"conv_stride\": 1,\n",
    "        #\"conv_groups\": params[\"conv_groups\"],\n",
    "        \"2d\": dim_2,\n",
    "    }\n",
    "elif net_type == \"SNN\":\n",
    "# for feedforward SNN\n",
    "    search_space = OrderedDict({\n",
    "        #\"num_hidden\": [512, 1024, 2048],\n",
    "        \"num_hidden\": [512, 1024, 2048],\n",
    "        #\"num_hidden_l2\": [1024, 512],\n",
    "        #\"num_layers\": [1, 2],\n",
    "        \"beta\": [0.9, 0.7, 0.5],\n",
    "        \"spike_grad\": [None, surrogate.fast_sigmoid(slope=10), surrogate.fast_sigmoid(slope=25), surrogate.fast_sigmoid(slope=50)],\n",
    "        \"bias\": [True, False],\n",
    "        \"learning_rate\": [1e-3, 1e-4, 1e-5],\n",
    "        \"out_num\": [2, 10, 20, 50],\n",
    "        #\"loss_type\": ['ce_mem', 'rate_loss', 'count_loss'],\n",
    "        \"num_steps\": [5, 10, 20, 50]\n",
    "    })\n",
    "    net_config = {\n",
    "        \"beta\": 0.9,\n",
    "        \"spike_grad\": None,\n",
    "        \"num_hidden\": 1024,\n",
    "        \"num_steps\": 10,\n",
    "        \"slope\": slope,\n",
    "        \"encoding\": 'rate',\n",
    "        \"out_num\": 2,\n",
    "        \"2d\": dim_2,\n",
    "        \"learning_rate\": 1e-4,\n",
    "    }\n",
    "elif net_type == \"DSNN\":\n",
    "# for feedforward SNN\n",
    "    search_space = OrderedDict({\n",
    "        #\"num_hidden\": [512, 1024, 2048],\n",
    "        \"num_hidden\": [1024, 2048],\n",
    "        \"num_hidden_l2\": [1024, 512, 256],\n",
    "        #\"num_layers\": [1, 2],\n",
    "        \"beta\": [0.9, 0.7, 0.5],\n",
    "        \"spike_grad\": [None, surrogate.fast_sigmoid(slope=10), surrogate.fast_sigmoid(slope=25), surrogate.fast_sigmoid(slope=50)],\n",
    "        \"bias\": [True, False],\n",
    "        \"learning_rate\": [1e-3, 1e-4, 1e-5],\n",
    "        \"out_num\": [2, 10, 20, 50],\n",
    "        #\"loss_type\": ['ce_mem', 'rate_loss', 'count_loss'],\n",
    "        \"num_steps\": [5, 10, 20, 50]\n",
    "    })\n",
    "    net_config = {\n",
    "        \"beta\": 0.9,\n",
    "        \"spike_grad\": None,\n",
    "        \"num_hidden\": 1024,\n",
    "        \"num_steps\": 10,\n",
    "        \"slope\": slope,\n",
    "        \"encoding\": 'rate',\n",
    "        \"out_num\": 2,\n",
    "        \"2d\": dim_2,\n",
    "        \"learning_rate\": 1e-4,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "keys = list(search_space.keys())\n",
    "combinations = list(itertools.product(*search_space.values()))\n",
    "random.shuffle(combinations)\n",
    "combinations = combinations[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = [\"bias\", \"beta\", \"learning_rate\", \"out_num\", \"num_steps\", \"num_hidden\", \"num_hidden_l2\"]\n",
    "train_params = [\"num_steps\", \"learning_rate\"], #\"loss_type\"]\n",
    "print(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, values in enumerate(combinations):\n",
    "    params = dict(zip(keys, values))\n",
    "    print(f\"\\n=== Trial {i + 1}/{len(combinations)} ===\")\n",
    "    #print(\"Params:\", params)\n",
    "    for key, value in params.items():\n",
    "        if key =='spike_grad':\n",
    "            if value is not None:\n",
    "                slope = value.__closure__[0].cell_contents\n",
    "                print('spike_grad: fast_sigmoid -', slope, flush=True)\n",
    "                net_config['slope'] = slope\n",
    "            else:\n",
    "                print('spike_grad: arctan', flush=True)\n",
    "        else:\n",
    "            print(key, ':', value, flush=True)\n",
    "        if key in net_params:\n",
    "            net_config[key] = value\n",
    "        if key in train_params:\n",
    "            train_config[key] = value\n",
    "\n",
    "    net_config[\"input_size\"] = 1024 if data_config['mix'] else num_bits\n",
    "\n",
    "    if net_type == \"CSNN\":\n",
    "        #net_config['conv_stride'] = [1 for _ in range(net_config['num_conv'])]\n",
    "        net_config['conv_stride'] = [params['conv_stride'] for _ in range(net_config['num_conv'])]\n",
    "        net_config['conv_kernel'] = params[\"conv_kernel\"]\n",
    "        net_config['conv_groups'] = params[\"conv_groups\"]\n",
    "        net_config['pool_size'] = params[\"pool_size\"]\n",
    "    \n",
    "    pop_coding = net_config['out_num'] > 2\n",
    "    train_config['loss_type'] = loss_type\n",
    "    train_config['prediction_fn'] = get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding)\n",
    "\n",
    "    net, train_net, val_net, test_net = get_spiking_net(net_type, net_config)\n",
    "    net = net.to(device)\n",
    "    train_config['val_net'] = val_net\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    train_config[\"scheduler\"] = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_config['num_epochs'])\n",
    "    \n",
    "    # DATA SPLIT\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, data_config=data_config, seed=seed, dtype=dtype)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "        \n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # LOSS FN\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1], dtype=np.int8), y=np.array(train_label, dtype=np.int8))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "    train_config[\"loss_fn\"] = get_loss_fn(loss_type=train_config[\"loss_type\"], class_weights=class_weights, pop_coding=pop_coding)\n",
    "\n",
    "\n",
    "    # TRAINING\n",
    "    start_time = time.time()\n",
    "    net, loss_hist, val_acc_hist, val_auc_hist, net_list, best_val_net = train_net(net=net, optimizer=optimizer, train_loader=train_loader, val_loader=val_loader, train_config=train_config, net_config=net_config)\n",
    "    end_time = time.time()\n",
    "    train_time = end_time - start_time\n",
    "    print()\n",
    "    print(f\"Time: {train_time:.4f} seconds\")\n",
    "    all_preds, all_targets = test_net(net, device, test_loader, train_config)\n",
    "    auc_roc_test = roc_auc_score(all_targets, all_preds)\n",
    "    print('Last model AUC on test set:', auc_roc_test)\n",
    "    model = net\n",
    "    model.load_state_dict(best_val_net)\n",
    "    all_preds, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    auc_roc_test = roc_auc_score(all_targets, all_preds)\n",
    "    print('Best model AUC on test set:', auc_roc_test)\n",
    "\n",
    "    \"\"\"     model = net\n",
    "    ensemble_preds =  np.zeros_like(all_preds)   \n",
    "    print(\"Ensemble models:\")\n",
    "    for state_dict in net_list:\n",
    "        model.load_state_dict(state_dict)\n",
    "        all_preds, _ = test_net(net, device, test_loader, train_config)\n",
    "        auc_roc_test = roc_auc_score(all_targets, all_preds)\n",
    "        print(\"....AUC:\",auc_roc_test)\n",
    "        ensemble_preds += all_preds\n",
    "    ensemble_preds = (ensemble_preds >= 3).astype(int)\n",
    "    auc_roc_test_ensemble = roc_auc_score(all_targets, ensemble_preds)\n",
    "    print('ensemble AUC on test set:', auc_roc_test_ensemble)\n",
    "    \"\"\"\n",
    "\n",
    "    result_entry = {\n",
    "        \"params\": params,\n",
    "        \"auc_test\": auc_roc_test,\n",
    "    }\n",
    "    results.append(result_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_results = results[6:]\n",
    "param_results.sort(key=lambda x: x[\"auc_test\"], reverse=True)\n",
    "print(\"\\nTop Configs:\")\n",
    "for r in param_results:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
