{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from snn_model import get_loss_fn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp,smiles_to_descriptor,smiles_to_onehot,smiles_to_onehot_selfies,data_splitter,get_spiking_net,make_filename\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from csnn_model import get_prediction_fn\n",
    "from snntorch import surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = f\"./results/logs/output_{timestamp}.txt\"\n",
    "log_file = open(log_path, \"w\")\n",
    "sys.stdout = log_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[1]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n",
    "\n",
    "for t in targets:\n",
    "    df_temp = df[[t, 'smiles']].dropna()\n",
    "    class_counts = df[t].count()\n",
    "    class_sum = df[t].sum()\n",
    "    print(t, class_counts, round(class_sum/class_counts, 2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "    # SR-MMP\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders 1427 samples, 0.52 class ratio\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Molecular Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"fp\", \"descriptor\", \"SELFIES-1hot\", \"SMILES-1hot\"]#, \"graph-list\"]\n",
    "\n",
    "repr_type = representations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repr_type == \"fp\":\n",
    "    fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['count_morgan', 1024], ['pubchem', 881]]\n",
    "    mix = False\n",
    "    fp_type, num_bits = fp_types[0]\n",
    "    if mix and fp_type == 'RDKit':\n",
    "        num_bits = 512\n",
    "    data_config = {\"fp_type\": fp_type,\n",
    "                \"num_bits\": num_bits,\n",
    "                \"radius\": 2,\n",
    "                \"fp_type_2\": fp_types[0][0],\n",
    "                \"num_bits_2\": 1024 - num_bits,\n",
    "                \"mix\": mix,}\n",
    "    dim_2 = False\n",
    "    print(fp_type, '-', num_bits)\n",
    "    if mix: print(data_config['fp_type_2'], '-', data_config['num_bits_2'])\n",
    "    if dim_2: print(\"2D FP\")\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    desc_type = [\"RDKit\", \"TODO\"]\n",
    "    data_config = {\"desc\": desc_type[0],\n",
    "                   \"size\": 0,\n",
    "                }\n",
    "elif repr_type == \"SELFIES-1hot\":\n",
    "    dim_2 = True\n",
    "    data_config = {}\n",
    "\n",
    "elif repr_type == \"SMILES-1hot\":\n",
    "    dim_2 = True\n",
    "    data_config = {}\n",
    "\n",
    "data_config[\"repr_type\"] = repr_type\n",
    "print(repr_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:54:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:54:24] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "split = \"scaffold\"\n",
    "dataset = None\n",
    "\n",
    "if dirname != 'BBBP':\n",
    "    split = \"random\"\n",
    "    if repr_type == \"fp\":\n",
    "        fp_array, target_array = smile_to_fp(df, data_config=data_config, target_name=target_name)\n",
    "        # Create Torch Dataset\n",
    "        fp_tensor = torch.tensor(fp_array, dtype=dtype)\n",
    "        print(fp_tensor.size())\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "        if dim_2:\n",
    "            fp_tensor = fp_tensor.view(-1, 32, 32)\n",
    "            print(fp_tensor.size())\n",
    "        dataset = TensorDataset(fp_tensor, target_tensor)\n",
    "    elif repr_type == \"descriptor\":\n",
    "        desc_array, target_array = smiles_to_descriptor(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        desc_tensor = torch.tensor(desc_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(desc_tensor, target_tensor)\n",
    "        print(desc_tensor.size())\n",
    "    elif repr_type == \"SELFIES-1hot\":\n",
    "        selfies_array, target_array = smiles_to_onehot_selfies(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        selfies_tensor = torch.tensor(selfies_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(selfies_tensor, target_tensor)\n",
    "        print(selfies_tensor.size())\n",
    "    elif repr_type == \"SMILES-1hot\":\n",
    "        smiles_array, target_array = smiles_to_onehot(df, data_config=data_config, target_name=target_name, missing_val=0)\n",
    "        # Create Torch Dataset\n",
    "        smiles_tensor = torch.tensor(smiles_array, dtype=dtype)\n",
    "        target_tensor = torch.tensor(target_array, dtype=dtype).long()\n",
    "\n",
    "        dataset = TensorDataset(smiles_tensor, target_tensor)\n",
    "        print(smiles_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repr_type == \"SMILES_1hot\":\n",
    "    longest_smiles = df.loc[df['smiles'].str.len().idxmax(), 'smiles']\n",
    "    print(longest_smiles)\n",
    "\n",
    "    from rdkit import Chem\n",
    "\n",
    "    mol = Chem.MolFromSmiles(longest_smiles)\n",
    "    print(\"Valid\" if mol else \"Invalid\")\n",
    "    cc = df[df['smiles'].str.len() > 256]\n",
    "\n",
    "    print(len(cc))\n",
    "    sample1 = smiles_array[0]\n",
    "    print(sample1)\n",
    "    print(selfies_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repr_type == \"descriptor\":\n",
    "    from rdkit.Chem import  Descriptors\n",
    "    print(\"desc_array Has NaNs:\", np.isnan(desc_array).any())\n",
    "    print(\"desc_array Has Infs:\", np.isinf(desc_array).any())\n",
    "    print(\"desc_tensor has nans:\", torch.isnan(desc_tensor).any().item())\n",
    "    print(\"desc_tensor has infs:\", torch.isinf(desc_tensor).any().item())\n",
    "\n",
    "    print(\"Max value in desc_array:\", np.max(desc_array))\n",
    "\n",
    "    # Find the index of the max value in the array\n",
    "    max_idx = np.argmax(desc_array)  # Returns the index of the max value in flattened array\n",
    "\n",
    "    # Find the corresponding row and descriptor index\n",
    "    row_idx = max_idx // desc_array.shape[1]  # Row index (which molecule)\n",
    "    desc_idx = max_idx % desc_array.shape[1]  # Descriptor index (which descriptor)\n",
    "    print(f\"Max value at row {row_idx}, descriptor {desc_idx} with value: {desc_array[row_idx, desc_idx]}\")\n",
    "\n",
    "    for k, (nm, fn) in enumerate(Descriptors._descList):\n",
    "        print(k, nm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "loss_types = ['ce_mem', 'rate_loss', 'count_loss', 'temporal_loss', 'bce_loss']\n",
    "loss_type = loss_types[2]\n",
    "print(loss_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_types = [\"SNN\", \"DSNN\", \"CSNN\", \"RSNN\"]\n",
    "net_type = net_types[2]\n",
    "slope = 10\n",
    "spike_grad = surrogate.fast_sigmoid(slope=slope)\n",
    "#spike_grad = None\n",
    "beta = 0.95 \n",
    "bias = True\n",
    "net_config = {\n",
    "            \"num_hidden\": 1024,\n",
    "            \"num_hidden_l2\": 256,\n",
    "            \"num_steps\": 10,\n",
    "            \"spike_grad\": spike_grad,\n",
    "            \"slope\": None if not spike_grad else slope, #spike_grad.__closure__[0].cell_contents,\n",
    "            \"beta\": beta,\n",
    "            \"encoding\": 'rate' if loss_type != 'temporal_loss' else 'ttfs',\n",
    "            \"bias\": bias,\n",
    "            \"out_num\": 2\n",
    "            }\n",
    "#print(spike_grad.__closure__[0].cell_contents)\n",
    "if net_type == \"CSNN\":\n",
    "    net_config['num_conv'] = 1\n",
    "    net_config['conv_stride'] = [1 for _ in range(net_config['num_conv'])]\n",
    "    net_config[\"pool_size\"] = 2\n",
    "    net_config[\"conv_kernel\"] = 3\n",
    "    #net_config[\"conv_stride\"] = 1\n",
    "    net_config[\"conv_groups\"] = 1\n",
    "\n",
    "if repr_type == \"fp\":\n",
    "    net_config[\"input_size\"] = 1024 if data_config['mix'] else num_bits\n",
    "    net_config[\"2d\"] = dim_2\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    net_config[\"input_size\"] = desc_tensor.shape[1]\n",
    "    net_config[\"2d\"] = False\n",
    "    net_config[\"time_steps\"] = 10\n",
    "\n",
    "if repr_type == \"SELFIES-1hot\":\n",
    "    net_config[\"input_size\"] = [desc_tensor.shape[1],desc_tensor.shape[2]] \n",
    "    net_config[\"2d\"] = True\n",
    "if repr_type == \"SMILES-1hot\":\n",
    "    net_config[\"2d\"] = True\n",
    "    net_config[\"input_size\"] = [desc_tensor.shape[1],desc_tensor.shape[2]] \n",
    "print(net_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pop_coding = net_config['out_num'] > 2\n",
    "lr=1e-4 #1e-6 default for 1000 epochs. csnn requires higher\n",
    "iterations = 30\n",
    "weight_decay = 0 # 1e-5\n",
    "optim_type = 'Adam'\n",
    "#optim_type = 'SGD'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "batch_size = 16 #16, 8\n",
    "train_config = {\"num_epochs\": 1000,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['num_steps'],\n",
    "                'val_net': None,\n",
    "                'prediction_fn': get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding),\n",
    "                }\n",
    "drop_last = net_type == \"CSNN\"\n",
    "pin_memory = device == \"cuda\"\n",
    "save_csv = True\n",
    "save_models = True\n",
    "results = [[], [], [], [], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----Configuration-----\")\n",
    "print(data_config)\n",
    "print(net_config)\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "\n",
    "# Disable RDKit logging for the scaffold meeting\n",
    "RDLogger.DisableLog('rdApp.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(metrics_list, all_targets, all_preds):\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    \n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "# for CSNN\n",
    "if net_type == \"CSNN\":\n",
    "    search_space = OrderedDict({\n",
    "        \"bias\": [True, False],\n",
    "        \"beta\": [0.9, 0.7, 0.5],\n",
    "        \"learning_rate\": [1e-3, 1e-4, 1e-5],\n",
    "        \"out_num\": [2, 10, 20, 50],\n",
    "        \"spike_grad\": [None, surrogate.fast_sigmoid(slope=10), surrogate.fast_sigmoid(slope=25), surrogate.fast_sigmoid(slope=50)],\n",
    "        \"conv_groups\": [1, 2],\n",
    "        \"conv_kernel\": [3, 5, 7],\n",
    "        \"pool_size\": [2, 4],\n",
    "        \"conv_stride\": [1, 2],\n",
    "        #\"loss_fn\": ['ce_mem', 'rate_loss', 'count_loss'],\n",
    "        \"num_steps\": [5, 10, 20, 50]\n",
    "    })\n",
    "    net_config = {\n",
    "        \"beta\": 0.9,\n",
    "        \"spike_grad\": None,\n",
    "        \"slope\": slope,\n",
    "        \"encoding\": 'rate',\n",
    "        \"out_num\": 2,\n",
    "        'num_conv': 1,\n",
    "        #\"out_num\": params['out_num'],\n",
    "        #\"learning_rate\": 1e-4,\n",
    "        #\"pool_size\" : 2,\n",
    "        #\"conv_kernel\": params[\"conv_kernel\"],\n",
    "        #\"conv_stride\": 1,\n",
    "        #\"conv_groups\": params[\"conv_groups\"],\n",
    "        \"2d\": dim_2,\n",
    "    }\n",
    "else:\n",
    "# for feedforward SNN\n",
    "    search_space = OrderedDict({\n",
    "        #\"num_hidden\": [512, 1024, 2048],\n",
    "        \"num_hidden\": [1024, 2048],\n",
    "        #\"num_hidden_l2\": [1024, 512],\n",
    "        #\"num_layers\": [1, 2],\n",
    "        \"beta\": [0.9, 0.7, 0.5],\n",
    "        \"spike_grad\": [None, surrogate.fast_sigmoid(slope=10), surrogate.fast_sigmoid(slope=25), surrogate.fast_sigmoid(slope=50)],\n",
    "        \"bias\": [True, False],\n",
    "        \"learning_rate\": [1e-3, 1e-4, 1e-5],\n",
    "        \"out_num\": [2, 10, 20, 50],\n",
    "        #\"loss_type\": ['ce_mem', 'rate_loss', 'count_loss'],\n",
    "        \"num_steps\": [5, 10, 20, 50]\n",
    "    })\n",
    "    net_config = {\n",
    "        \"beta\": 0.9,\n",
    "        \"spike_grad\": None,\n",
    "        \"num_hidden\": 1024,\n",
    "        \"num_steps\": 10,\n",
    "        \"slope\": slope,\n",
    "        \"encoding\": 'rate',\n",
    "        \"out_num\": 2,\n",
    "        \"2d\": dim_2,\n",
    "        \"learning_rate\": 1e-4,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "keys = list(search_space.keys())\n",
    "combinations = list(itertools.product(*search_space.values()))\n",
    "random.shuffle(combinations)\n",
    "combinations = combinations[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = [\"bias\", \"beta\", \"learning_rate\", \"out_num\", \"num_steps\", \"num_hidden\", \"num_hidden_l2\"]\n",
    "train_params = [\"num_steps\", \"learning_rate\"], #\"loss_type\"]\n",
    "print(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m compute_class_weight(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8), y\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(train_label, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8))\n\u001b[0;32m     52\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(class_weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 53\u001b[0m train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_loss_fn(loss_type\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, class_weights\u001b[38;5;241m=\u001b[39mclass_weights, pop_coding\u001b[38;5;241m=\u001b[39mpop_coding)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# TRAINING\u001b[39;00m\n\u001b[0;32m     57\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss_type'"
     ]
    }
   ],
   "source": [
    "for i, values in enumerate(combinations):\n",
    "    params = dict(zip(keys, values))\n",
    "    print(f\"\\n=== Trial {i + 1}/{len(combinations)} ===\")\n",
    "    #print(\"Params:\", params)\n",
    "    for key, value in params.items():\n",
    "        if key =='spike_grad':\n",
    "            if value is not None:\n",
    "                slope = value.__closure__[0].cell_contents\n",
    "                print('spike_grad: fast_sigmoid -', slope, flush=True)\n",
    "                net_config['slope'] = slope\n",
    "            else:\n",
    "                print('spike_grad: arctan', flush=True)\n",
    "        else:\n",
    "            print(key, ':', value, flush=True)\n",
    "        if key in net_params:\n",
    "            net_config[key] = value\n",
    "        if key in train_params:\n",
    "            train_config[key] = value\n",
    "\n",
    "    net_config[\"input_size\"] = 1024 if data_config['mix'] else num_bits\n",
    "\n",
    "    if net_type == \"CSNN\":\n",
    "        #net_config['conv_stride'] = [1 for _ in range(net_config['num_conv'])]\n",
    "        net_config['conv_stride'] = [params['conv_stride'] for _ in range(net_config['num_conv'])]\n",
    "        net_config['conv_kernel'] = params[\"conv_kernel\"]\n",
    "        net_config['conv_groups'] = params[\"conv_groups\"]\n",
    "        net_config['pool_size'] = params[\"pool_size\"]\n",
    "    \n",
    "    pop_coding = net_config['out_num'] > 2\n",
    "    train_config['loss_type'] = loss_type\n",
    "    train_config['prediction_fn'] = get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding)\n",
    "\n",
    "    net, train_net, val_net, test_net = get_spiking_net(net_type, net_config)\n",
    "    net = net.to(device)\n",
    "    train_config['val_net'] = val_net\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    train_config[\"scheduler\"] = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_config['num_epochs'])\n",
    "    \n",
    "    # DATA SPLIT\n",
    "    train, val, test = data_splitter(df, target_name, split=split, dataset=dataset, data_config=data_config, seed=seed, dtype=dtype)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "        \n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # LOSS FN\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1], dtype=np.int8), y=np.array(train_label, dtype=np.int8))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "    train_config[\"loss_fn\"] = get_loss_fn(loss_type=train_config[\"loss_type\"], class_weights=class_weights, pop_coding=pop_coding)\n",
    "\n",
    "\n",
    "    # TRAINING\n",
    "    start_time = time.time()\n",
    "    net, loss_hist, val_acc_hist, val_auc_hist, net_list, best_val_net = train_net(net=net, optimizer=optimizer, train_loader=train_loader, val_loader=val_loader, train_config=train_config, net_config=net_config)\n",
    "    end_time = time.time()\n",
    "    train_time = end_time - start_time\n",
    "    print()\n",
    "    print(f\"Time: {train_time:.4f} seconds\")\n",
    "    all_preds, all_targets = test_net(net, device, test_loader, train_config)\n",
    "    auc_roc_test = roc_auc_score(all_targets, all_preds)\n",
    "    print('Last model AUC on test set:', auc_roc_test)\n",
    "    model = net\n",
    "    model.load_state_dict(best_val_net)\n",
    "    all_preds, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    auc_roc_test = roc_auc_score(all_targets, all_preds)\n",
    "    print('Best model AUC on test set:', auc_roc_test)\n",
    "\n",
    "    \"\"\"     model = net\n",
    "    ensemble_preds =  np.zeros_like(all_preds)   \n",
    "    print(\"Ensemble models:\")\n",
    "    for state_dict in net_list:\n",
    "        model.load_state_dict(state_dict)\n",
    "        all_preds, _ = test_net(net, device, test_loader, train_config)\n",
    "        auc_roc_test = roc_auc_score(all_targets, all_preds)\n",
    "        print(\"....AUC:\",auc_roc_test)\n",
    "        ensemble_preds += all_preds\n",
    "    ensemble_preds = (ensemble_preds >= 3).astype(int)\n",
    "    auc_roc_test_ensemble = roc_auc_score(all_targets, ensemble_preds)\n",
    "    print('ensemble AUC on test set:', auc_roc_test_ensemble)\n",
    "    \"\"\"\n",
    "\n",
    "    result_entry = {\n",
    "        \"params\": params,\n",
    "        \"auc_test\": auc_roc_test,\n",
    "    }\n",
    "    results.append(result_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_results = results[6:]\n",
    "param_results.sort(key=lambda x: x[\"auc_test\"], reverse=True)\n",
    "print(\"\\nTop Configs:\")\n",
    "for r in param_results:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
