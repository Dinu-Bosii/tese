{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n",
      "c:\\Users\\knsve\\Desktop\\MEI\\Tese\\torch\\snn_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from snn_model import get_loss_fn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_dataset_df, smile_to_fp,smiles_to_descriptor,smiles_to_onehot, smiles_to_onehot_selfies, data_splitter, get_spiking_net, make_filename\n",
    "from utils import smiles_to_feat\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from csnn_model import get_prediction_fn\n",
    "from snntorch import surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/scikit-fingerprints/scikit-fingerprints.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze\n",
    "#!pip install networkx==3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = f\"./results/logs/output_{timestamp}.txt\"\n",
    "log_file = open(log_path, \"w\")\n",
    "sys.stdout = log_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['tox21.csv','sider.csv', 'BBBP.csv']\n",
    "dt_file = files[0]\n",
    "dirname = dt_file.removesuffix('.csv')\n",
    "\n",
    "df, targets = load_dataset_df(filename=dt_file)\n",
    "\n",
    "for t in targets:\n",
    "    df_temp = df[[t, 'smiles']].dropna()\n",
    "    class_counts = df[t].count()\n",
    "    class_sum = df[t].sum()\n",
    "    print(t, class_counts, round(class_sum/class_counts, 2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dirname == 'tox21':\n",
    "    # SR-ARE\n",
    "    target_name = targets[7]\n",
    "    # SR-MMP\n",
    "elif dirname == 'sider':\n",
    "    #Hepatobiliary disorders 1427 samples, 0.52 class ratio\n",
    "    target_name = targets[0]\n",
    "else:\n",
    "    target_name = targets[0]\n",
    "    \n",
    "df = df[[target_name, 'smiles']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Molecular Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = [\"fp\", \"descriptor\", \"SELFIES-1hot\", \"SMILES-1hot\"]#, \"graph-list\"]\n",
    "\n",
    "repr_type = representations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repr_type == \"fp\":\n",
    "    fp_types = [['morgan', 1024], ['maccs', 167], ['RDKit', 1024], ['count_morgan', 1024], ['pubchem', 881]]\n",
    "    mix = False\n",
    "    fp_type, num_bits = fp_types[0]\n",
    "    if mix and fp_type == 'RDKit':\n",
    "        num_bits = 512\n",
    "    data_config = {\"fp_type\": fp_type,\n",
    "                \"num_bits\": num_bits,\n",
    "                \"radius\": 2,\n",
    "                \"fp_type_2\": fp_types[0][0],\n",
    "                \"num_bits_2\": 1024 - num_bits,\n",
    "                \"mix\": mix,\n",
    "                \"dim_2\": False}\n",
    "    dim_2 = data_config['dim_2']\n",
    "    print(fp_type, '-', num_bits)\n",
    "    if mix: print(data_config['fp_type_2'], '-', data_config['num_bits_2'])\n",
    "    if dim_2: print(\"2D FP\")\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    desc_type = [\"RDKit\", \"Mordred\"]\n",
    "    data_config = {\"desc_type\": desc_type[0],\n",
    "                   \"size\": 0,\n",
    "                }\n",
    "\n",
    "data_config[\"repr_type\"] = repr_type\n",
    "print(repr_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:28:16] Explicit valence for atom # 8 Al, 6, is greater than permitted\n",
      "[20:28:16] Explicit valence for atom # 3 Al, 6, is greater than permitted\n",
      "[20:28:16] Explicit valence for atom # 4 Al, 6, is greater than permitted\n",
      "[20:28:17] Explicit valence for atom # 4 Al, 6, is greater than permitted\n",
      "[20:28:17] Explicit valence for atom # 9 Al, 6, is greater than permitted\n",
      "[20:28:17] Explicit valence for atom # 5 Al, 6, is greater than permitted\n",
      "[20:28:18] Explicit valence for atom # 16 Al, 6, is greater than permitted\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "split = \"scaffold\" if dirname == \"BBBP\" else \"random\"\n",
    "dataset = None\n",
    "\n",
    "\n",
    "feat_tensor, target_tensor, feat_df = smiles_to_feat(df,repr_type=repr_type, data_config=data_config, target_name=target_name, dtype=dtype)\n",
    "print(feat_tensor.shape)\n",
    "dataset = TensorDataset(feat_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if repr_type == \"descriptor\":\\n    from rdkit.Chem import  Descriptors\\n    print(\"desc_array Has NaNs:\", np.isnan(feat_array).any())\\n    print(\"desc_array Has Infs:\", np.isinf(desc_array).any())\\n    print(\"feat_tensor has nans:\", torch.isnan(feat_tensor).any().item())\\n    print(\"feat_tensor has infs:\", torch.isinf(feat_tensor).any().item())\\n\\n    print(\"Max value in desc_array:\", np.max(desc_array))\\n\\n    # Find the index of the max value in the array\\n    max_idx = np.argmax(desc_array)  # Returns the index of the max value in flattened array\\n\\n    # Find the corresponding row and descriptor index\\n    row_idx = max_idx // desc_array.shape[1]  # Row index (which molecule)\\n    desc_idx = max_idx % desc_array.shape[1]  # Descriptor index (which descriptor)\\n    print(f\"Max value at row {row_idx}, descriptor {desc_idx} with value: {desc_array[row_idx, desc_idx]}\")\\n\\n    for k, (nm, fn) in enumerate(Descriptors._descList):\\n        print(k, nm) '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" if repr_type == \"descriptor\":\n",
    "    from rdkit.Chem import  Descriptors\n",
    "    print(\"desc_array Has NaNs:\", np.isnan(feat_array).any())\n",
    "    print(\"desc_array Has Infs:\", np.isinf(desc_array).any())\n",
    "    print(\"feat_tensor has nans:\", torch.isnan(feat_tensor).any().item())\n",
    "    print(\"feat_tensor has infs:\", torch.isinf(feat_tensor).any().item())\n",
    "\n",
    "    print(\"Max value in desc_array:\", np.max(desc_array))\n",
    "\n",
    "    # Find the index of the max value in the array\n",
    "    max_idx = np.argmax(desc_array)  # Returns the index of the max value in flattened array\n",
    "\n",
    "    # Find the corresponding row and descriptor index\n",
    "    row_idx = max_idx // desc_array.shape[1]  # Row index (which molecule)\n",
    "    desc_idx = max_idx % desc_array.shape[1]  # Descriptor index (which descriptor)\n",
    "    print(f\"Max value at row {row_idx}, descriptor {desc_idx} with value: {desc_array[row_idx, desc_idx]}\")\n",
    "\n",
    "    for k, (nm, fn) in enumerate(Descriptors._descList):\n",
    "        print(k, nm) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "loss_types = ['ce_mem', 'rate_loss', 'count_loss', 'temporal_loss', 'bce_loss']\n",
    "loss_type = loss_types[2]\n",
    "print(loss_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_types = [\"SNN\", \"DSNN\", \"CSNN\", \"RSNN\"]\n",
    "net_type = net_types[2]\n",
    "slope = 10\n",
    "#spike_grad = surrogate.fast_sigmoid(slope=slope)\n",
    "spike_grad = None\n",
    "beta = 0.7\n",
    "bias = True\n",
    "net_config = {\n",
    "            \"num_hidden\": 1024,\n",
    "            \"num_hidden_l2\": 256,\n",
    "            \"num_steps\": 5,\n",
    "            \"spike_grad\": spike_grad,\n",
    "            \"slope\": None if not spike_grad else slope, #spike_grad.__closure__[0].cell_contents,\n",
    "            \"beta\": beta,\n",
    "            \"encoding\": 'rate' if loss_type != 'temporal_loss' else 'ttfs',\n",
    "            \"bias\": bias,\n",
    "            \"out_num\": 2,\n",
    "            \"num_hidden_layers\": 2,\n",
    "            \"num_hidden_l3\": 256,\n",
    "            }\n",
    "if net_type == \"CSNN\":\n",
    "    \"\"\"     net_config['num_conv'] = 1\n",
    "    net_config[\"pool_size\"] = 4\n",
    "    net_config[\"conv_kernel\"] = 5\n",
    "    net_config[\"conv_stride\"] = [1 for _ in range(net_config['num_conv'])]\n",
    "    net_config[\"conv_groups\"] = 1 \"\"\"\n",
    "    net_config['num_conv'] = 1\n",
    "    net_config[\"pool_size\"] = 4\n",
    "    net_config[\"conv_kernel\"] = 3\n",
    "    net_config[\"conv_stride\"] = [1 for _ in range(net_config['num_conv'])]\n",
    "    net_config[\"conv_groups\"] = 1\n",
    "\n",
    "if repr_type == \"fp\":\n",
    "    net_config[\"input_size\"] = 1024 if data_config['mix'] else num_bits\n",
    "    net_config[\"2d\"] = data_config['dim_2']\n",
    "\n",
    "elif repr_type == \"descriptor\":\n",
    "    net_config[\"input_size\"] = feat_tensor.shape[1]\n",
    "    net_config[\"2d\"] = False\n",
    "    #net_config[\"num_steps\"] = 10\n",
    "\n",
    "print(net_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "pop_coding = net_config['out_num'] > 2\n",
    "lr=1e-4 #1e-6 default for 1000 epochs. csnn requires higher\n",
    "iterations = 30\n",
    "weight_decay = 0 # 1e-5\n",
    "#weight_decay = 1e-4\n",
    "optim_type = 'Adam'\n",
    "#optim_type = 'SGD'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "batch_size = 16 #16, 8\n",
    "train_config = {\"num_epochs\": 100,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"loss_type\": loss_type,\n",
    "                \"loss_fn\": None,\n",
    "                'dtype': dtype,\n",
    "                'num_steps': net_config['num_steps'],\n",
    "                'val_net': None,\n",
    "                'prediction_fn': get_prediction_fn(encoding=net_config['encoding'], pop_coding=pop_coding),\n",
    "                }\n",
    "drop_last = net_type == \"CSNN\"\n",
    "pin_memory = device == \"cuda\"\n",
    "save_csv = True\n",
    "save_models = True\n",
    "results = [[], [], [], [], [], []]\n",
    "print(train_config[\"prediction_fn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----Configuration-----\")\n",
    "print(data_config)\n",
    "print(net_config)\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "\n",
    "# Disable RDKit logging for the scaffold meeting\n",
    "RDLogger.DisableLog('rdApp.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(metrics_list, all_targets, all_preds):\n",
    "    auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "    \n",
    "    all_preds = np.array(all_preds) >= 0.0\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_preds).ravel()\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    \n",
    "    metrics_list[0].append(accuracy)\n",
    "    metrics_list[1].append(auc_roc)\n",
    "    metrics_list[2].append(sensitivity)\n",
    "    metrics_list[3].append(specificity)\n",
    "    metrics_list[4].append(f1)\n",
    "    metrics_list[5].append(precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_norm(train_subset, val_subset, test_subset):\n",
    "    train_tensor, _ = train_subset[:]\n",
    "    val_tensor, _ = val_subset[:]\n",
    "    test_tensor, _ = test_subset[:]\n",
    "\n",
    "    mean = train_tensor.mean(dim=0)\n",
    "    std = train_tensor.std(dim=0)\n",
    "    std = std.clamp(min=1e-6)\n",
    "    #print(mean.size())\n",
    "    train_norm = (train_tensor - mean)\n",
    "    #print(torch.isnan(train_tensor).any())\n",
    "    #print(torch.isnan(train_norm).any())\n",
    "    train_norm = train_norm / std\n",
    "    #print(torch.isnan(train_norm).any())\n",
    "    val_norm = (val_tensor - mean) / std\n",
    "    test_norm = (test_tensor - mean) / std\n",
    "\n",
    "    return train_norm, val_norm, test_norm\n",
    "\n",
    "def minmax_norm(train_subset, val_subset, test_subset):\n",
    "    train_tensor, _ = train_subset[:]\n",
    "    val_tensor, _ = val_subset[:]\n",
    "    test_tensor, _ = test_subset[:]\n",
    "\n",
    "    min_val = train_tensor.min(dim=0).values\n",
    "    max_val = train_tensor.max(dim=0).values\n",
    "    range_val = (max_val - min_val).clamp(min=1e-6)\n",
    "\n",
    "    train_norm = ((train_tensor - min_val) / range_val).clamp(0.0, 1.0)\n",
    "    val_norm   = ((val_tensor   - min_val) / range_val).clamp(0.0, 1.0)\n",
    "    test_norm  = ((test_tensor  - min_val) / range_val).clamp(0.0, 1.0)\n",
    "\n",
    "    return train_norm, val_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_in_train = False\n",
    "print(\"Validation Set used in training:\", val_in_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39mremovesuffix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     72\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(best_val_net, model_name)\n\u001b[1;32m---> 74\u001b[0m \u001b[43mcalc_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds_best\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_targets_last\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m, in \u001b[0;36mcalc_metrics\u001b[1;34m(metrics_list, all_targets, all_preds)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalc_metrics\u001b[39m(metrics_list, all_targets, all_preds):\n\u001b[0;32m      2\u001b[0m     auc_roc \u001b[38;5;241m=\u001b[39m roc_auc_score(all_targets, all_preds)\n\u001b[1;32m----> 4\u001b[0m     all_preds \u001b[38;5;241m=\u001b[39m (\u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      5\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(all_targets, all_preds)\n\u001b[0;32m      6\u001b[0m     tn, fp, fn, tp \u001b[38;5;241m=\u001b[39m confusion_matrix(all_targets, all_preds)\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "for iter in range(iterations):\n",
    "    print(f\"Iteration:{iter + 1}/{iterations}\")\n",
    "    seed = iter + 1\n",
    "    print(f\"Seed:{seed}\")\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    net, train_net, val_net, test_net = get_spiking_net(net_type, net_config)\n",
    "    net = net.to(device)\n",
    "    train_config['val_net'] = val_net\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    #optimizer = torch.optim.AdamW(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    #optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    #optimizer = torch.optim.Adamax(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=weight_decay)\n",
    "    train_config[\"scheduler\"] = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_config['num_epochs'], eta_min=1e-6)\n",
    "    \n",
    "    # DATA SPLIT\n",
    "    train, val, test = data_splitter(feat_df, target_name, split=split, dataset=dataset, data_config=data_config, seed=seed, dtype=dtype)\n",
    "    _, train_label = train[:]\n",
    "    _, val_label = val[:]\n",
    "    _, test_label = test[:]\n",
    "        \n",
    "    if repr_type == \"descriptor\":\n",
    "        train_data, val_data, test_data = minmax_norm(train, val, test)\n",
    "        train = TensorDataset(train_data, train_label)\n",
    "        val = TensorDataset(val_data,val_label)\n",
    "        test = TensorDataset(test_data, test_label)\n",
    "\n",
    "    if val_in_train:\n",
    "        train_feat, _ = train[:]\n",
    "        val_feat, _ = val[:]\n",
    "        train = TensorDataset(\n",
    "        torch.cat([train_feat, val_feat], dim=0),\n",
    "        torch.cat([train_label, val_label], dim=0)\n",
    "        )\n",
    "    \n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    # LOSS FN\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1], dtype=np.int8), y=np.array(train_label, dtype=np.int8))\n",
    "    #class_weights[0] = class_weights[0]/2 \n",
    "    class_weights[1] = class_weights[1]*2\n",
    "    print(\"class weights:\", class_weights)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "    train_config[\"loss_fn\"] = get_loss_fn(loss_type=loss_type, class_weights=class_weights, pop_coding=pop_coding)\n",
    "    train_config[\"test_loader\"] = test_loader\n",
    "\n",
    "    # TRAINING\n",
    "    start_time = time.time()\n",
    "    net, loss_hist, val_acc_hist, val_auc_hist, net_list, best_val_net = train_net(net=net, optimizer=optimizer, train_loader=train_loader, val_loader=val_loader, train_config=train_config, net_config=net_config)\n",
    "    end_time = time.time()\n",
    "    train_time = end_time - start_time\n",
    "    times.append(train_time)\n",
    "    print()\n",
    "    print(f\"Time: {train_time:.4f} seconds\")\n",
    "    # TESTING\n",
    "    all_preds_last, all_targets_last = test_net(net, device, test_loader, train_config)\n",
    "    auc_roc_test = roc_auc_score(all_targets_last, all_preds_last)\n",
    "    print('Last model AUC on test set:', auc_roc_test)\n",
    "    model = net\n",
    "    model.load_state_dict(best_val_net)\n",
    "    all_preds_best, all_targets = test_net(model, device, test_loader, train_config)\n",
    "    auc_roc_test = roc_auc_score(all_targets, all_preds_best)\n",
    "    print('Best model AUC on test set:', auc_roc_test)\n",
    "\n",
    "    if save_models:\n",
    "        filename = make_filename(dirname, target_name, net_type, data_config, lr, weight_decay, optim_type, net_config, train_config, model, model = True)\n",
    "        model_name = filename.removesuffix('.csv') + f\"seed-{seed}\" +'.pth'\n",
    "        torch.save(best_val_net, model_name)\n",
    "\n",
    "    calc_metrics(results, all_preds=all_preds_best, all_targets=all_targets_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(times)/len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" model = net\n",
    "best_test_auc = 0\n",
    "best_epoch = 0\n",
    "for index, model_dict in enumerate(net_list):\n",
    "    model.load_state_dict(model_dict)\n",
    "    model.to(device)\n",
    "    all_preds2, all_targets2 = test_net(model, device, test_loader, train_config)\n",
    "    auc_roc_test = roc_auc_score(all_targets2, all_preds2)\n",
    "    if auc_roc_test > best_test_auc:\n",
    "        best_test_auc, best_epoch = (auc_roc_test, index)\n",
    "\n",
    "print('-- best epoch:', best_epoch,'--best auc:', best_test_auc) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothed Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "#from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "#print(loss_hist[len(loss_hist) - 5:len(loss_hist)])\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "#plt.plot(np.convolve(loss_hist, np.ones(30)/30, mode='valid'))\n",
    "#plt.plot(savgol_filter(loss_hist, window_length=100, polyorder=3))\n",
    "#plt.plot(lowess(loss_hist, np.arange(len(loss_hist)), frac=0.1)[:, 1])\n",
    "plt.plot(gaussian_filter1d(loss_hist, sigma=6))\n",
    "#plt.plot(loss_hist)\n",
    "#plt.axhline(y=1, color='r', linestyle='--', label='y = 1')\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = train_config['num_epochs']\n",
    "num_minibatches_per_epoch = len(loss_hist) // num_epochs\n",
    "\n",
    "# Create x-axis values in terms of epochs\n",
    "epochs = np.linspace(1, num_epochs, len(loss_hist))\n",
    "epoch_losses = np.array(loss_hist).reshape(num_epochs, num_minibatches_per_epoch).mean(axis=1)\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, label=\"Loss per Epoch\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Set\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "\n",
    "#plt.plot(gaussian_filter1d(val_auc_hist, sigma=6))\n",
    "plt.plot(val_auc_hist)\n",
    "plt.title(\"ROC AUC on Validation Set\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_np = np.zeros(12)\n",
    "\n",
    "for i, metric in enumerate(results):\n",
    "    metrics_np[i*2] = np.round(np.mean(metric), 3)\n",
    "    metrics_np[i*2+1] = np.round(np.std(metric), 3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy:  {metrics_np[0]:.3f} ± {metrics_np[1]:.3f}\")\n",
    "print(f\"AUC ROC: {metrics_np[2]:.3f} ± {metrics_np[3]:.3f}\")\n",
    "print(f\"Sensitivity: {metrics_np[4]:.3f} ± {metrics_np[5]:.3f}\")\n",
    "print(f\"Specificity: {metrics_np[6]:.3f} ± {metrics_np[7]:.3f}\")\n",
    "\n",
    "metric_names = ['Acc', 'AUC', 'Sn', 'Sp', 'F1', 'Precision']\n",
    "metrics_np = metrics_np.reshape(1, -1)\n",
    "columns = []\n",
    "for name in metric_names:\n",
    "    columns.extend([f'Mean {name}', f'Std {name}'])\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_np, columns=columns)\n",
    "num_hidden = net_config['num_hidden']\n",
    "time_steps = train_config['num_steps']\n",
    "num_epochs = train_config['num_epochs']\n",
    "\n",
    "# TODO: Add neuron thresholds to name\n",
    "filename = make_filename(dirname, target_name, net_type, data_config, lr, weight_decay, optim_type, net_config, train_config, model)\n",
    "if save_csv: df_metrics.to_csv(filename, index=False)\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_auc = np.argmin(results[1])\n",
    "print(\"min auc:\", results[1][min_auc], \"at\", min_auc)\n",
    "\n",
    "max_auc = np.argmax(results[1])\n",
    "print(\"max auc:\", results[1][max_auc], \"at\", max_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
